{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7d006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_FNS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43badf",
   "metadata": {},
   "source": [
    "### Reward functions for Walker2d-v5\n",
    "\n",
    "In this project, we want to study the effect of different reward functions for the `Walker2d-v5` environment (Gymnasium / MuJoCo). In the base version, the reward at each step is the sum of three terms:\n",
    "\n",
    "* **healthy_reward**: a survival bonus when the robot stays within a zone considered “healthy”;\n",
    "* **forward_reward**: a reward proportional to forward velocity (movement in x per unit of time);\n",
    "* **ctrl_cost**: a quadratic cost on the actions (penalizes actions with too much torque).\n",
    "\n",
    "The total reward is therefore:\n",
    "\n",
    "> reward = healthy_reward + forward_reward − ctrl_cost\n",
    "\n",
    "and `info` contains the individual terms under the keys\n",
    "`\"reward_forward\"`, `\"reward_ctrl\"`, `\"reward_survive\"`.\n",
    "(See the official `Walker2d-v5` documentation.)\n",
    "\n",
    "In the context of *reward shaping*, we modify this reward function to guide learning, for example by adding posture terms, target speed, or action smoothing. This type of modification is studied theoretically in the classic paper by Ng, Harada, and Russell, *\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\"* (ICML 1999), which shows in which cases certain reward transformations preserve the optimal policy.\n",
    "\n",
    "The code below implements several reward variants for `Walker2d-v5` in the form of a Gymnasium wrapper, so that a reward function can be easily selected and an agent trained with it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26155d91",
   "metadata": {},
   "source": [
    "### Reward function 1: speed – energy – survival\n",
    "\n",
    "In the `Walker2d-v5` environment (Gymnasium / MuJoCo), the default reward is split into three terms:\n",
    "\n",
    "* `reward_forward`: reward for forward speed;\n",
    "* `reward_ctrl`: control cost (negative), proportional to the squared actions;\n",
    "* `reward_survive`: survival bonus as long as the robot stays in a “healthy” state.\n",
    "\n",
    "See the `Walker2d-v5` documentation, which explains this reward decomposition in the `info` dictionary.\n",
    "\n",
    "For our first reward function, we define a linear combination of these three terms:\n",
    "\n",
    "$\n",
    "r_t = w_{\\text{forward}} \\cdot \\text{reward_forward}\n",
    "+ w_{\\text{ctrl}} \\cdot \\text{reward_ctrl}\n",
    "+ w_{\\text{survive}} \\cdot \\text{reward_survive}\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* $w_{\\text{forward}}$ controls the importance of speed;\n",
    "* $w_{\\text{ctrl}}$ controls the importance of energy consumption (since `reward_ctrl` is negative, a positive weight means “we penalize energy use”);\n",
    "* $w_{\\text{survive}}$ controls the importance of the survival bonus.\n",
    "\n",
    "By varying these weights, we can study the trade-off between **speed**, **energy consumption**, and **stability** (survival) of the walker, which is especially interesting in a context with perturbations (observation noise, randomization of initial conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73dce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def reward_speed_energy(\n",
    "    info: Dict[str, Any],\n",
    "    w_forward: float = 1.0,\n",
    "    w_ctrl: float = 1.0,\n",
    "    w_survive: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fonction de récompense 1 : combinaison vitesse / énergie / survie\n",
    "    pour Walker2d-v5.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    info : dict\n",
    "        Le dictionnaire `info` renvoyé par env.step(action).\n",
    "        On suppose qu'il contient les clés :\n",
    "        - \"reward_forward\"\n",
    "        - \"reward_ctrl\"\n",
    "        - \"reward_survive\"\n",
    "    w_forward : float\n",
    "        Poids de la récompense de vitesse (reward_forward).\n",
    "    w_ctrl : float\n",
    "        Poids du coût de contrôle (reward_ctrl).\n",
    "        Attention : reward_ctrl est déjà négatif dans Walker2d.\n",
    "        Un w_ctrl > 0 correspond donc à une pénalisation de l'énergie.\n",
    "    w_survive : float\n",
    "        Poids du bonus de survie (reward_survive).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    float\n",
    "        La récompense scalaire r_t.\n",
    "    \"\"\"\n",
    "    forward = float(info.get(\"reward_forward\", 0.0))\n",
    "    ctrl = float(info.get(\"reward_ctrl\", 0.0))         # déjà négatif\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    reward = (\n",
    "        w_forward * forward\n",
    "        + w_ctrl * ctrl\n",
    "        + w_survive * survive\n",
    "    )\n",
    "    return reward\n",
    "\n",
    "# obs, base_reward, terminated, truncated, info = env.step(action)\n",
    "# new_reward = reward_speed_energy(\n",
    "#     info,\n",
    "#     w_forward=1.0,\n",
    "#     w_ctrl=1.0,\n",
    "#     w_survive=1.0,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ef535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- 1) Wrapper qui remplace la récompense par reward_speed_energy ---\n",
    "\n",
    "class SpeedEnergyRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, w_forward=1.0, w_ctrl=1.0, w_survive=1.0):\n",
    "        super().__init__(env)\n",
    "        self.w_forward = w_forward\n",
    "        self.w_ctrl = w_ctrl\n",
    "        self.w_survive = w_survive\n",
    "\n",
    "    def step(self, action):\n",
    "        # on appelle l'env \"normal\"\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # on calcule NOTRE récompense à partir de info\n",
    "        new_reward = reward_speed_energy(\n",
    "            info,\n",
    "            w_forward=self.w_forward,\n",
    "            w_ctrl=self.w_ctrl,\n",
    "            w_survive=self.w_survive,\n",
    "        )\n",
    "\n",
    "        # on renvoie obs, new_reward (et pas base_reward)\n",
    "        return obs, new_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --- 2) Création de l'environnement + enregistrement vidéo ---\n",
    "\n",
    "video_folder = \"./videos_speed_energy\"\n",
    "\n",
    "# IMPORTANT : render_mode=\"rgb_array\" pour pouvoir faire une vidéo\n",
    "env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "# on applique notre wrapper de récompense\n",
    "env_wrapped = SpeedEnergyRewardWrapper(\n",
    "    env_base,\n",
    "    w_forward=1.0,\n",
    "    w_ctrl=1.0,\n",
    "    w_survive=1.0,\n",
    ")\n",
    "\n",
    "# on ajoute le wrapper vidéo\n",
    "env = RecordVideo(\n",
    "    env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-speed_energy\",\n",
    "    episode_trigger=lambda ep_id: True,  # filme tous les épisodes\n",
    "    video_length=0,                       # 0 = filme l'épisode complet\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3) Entraînement rapide avec SAC ---\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# nombre de pas tout petit pour tester (à augmenter plus tard)\n",
    "model.learn(total_timesteps=5_000)\n",
    "\n",
    "\n",
    "# --- 4) On filme un épisode avec le modèle entraîné ---\n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "print(f\"Épisode terminé. Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508bb2c",
   "metadata": {},
   "source": [
    "### Reward function 2: target speed\n",
    "\n",
    "The idea of this reward is no longer “faster = better,” but instead “the robot must walk at a **target speed** $v^*$,” for example $v^* \\in {1.0, 2.0}$ m/s.\n",
    "\n",
    "We define:\n",
    "\n",
    "* $v_x$: the horizontal velocity of the torso (in `Walker2d-v5`, this is `obs[8]`, the torso velocity in x).\n",
    "* $a_t$: the action vector (torques) at time $t$,\n",
    "* `reward_survive`: the survival bonus provided by the environment.\n",
    "\n",
    "The reward is defined as:\n",
    "\n",
    "$ r_t = - \\alpha\\, |v_x - v^*| - \\beta \\, \\|a_t\\|^2 + w_{\\text{survive}} \\cdot \\text{reward\\_survive} $\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\alpha$ controls the importance of staying close to the target speed $v^*$,\n",
    "* $\\beta$ controls how strongly energy use is penalized (squared norm of the actions),\n",
    "* $w_{\\text{survive}}$ adjusts the importance of the survival term.\n",
    "\n",
    "#### Interest under perturbations\n",
    "\n",
    "When noise is added (to observations or initial conditions), simply maximizing speed often pushes the agent to **run faster and faster**, with jerky and unstable movements.\n",
    "\n",
    "With this **target speed** reward function:\n",
    "\n",
    "* the agent is encouraged to maintain a **steady speed** close to $v^*$,\n",
    "* we can measure:\n",
    "\n",
    "  * the variance of the speed $v_x$ during an episode,\n",
    "  * the variance of the actions,\n",
    "  * the energy consumed ($\\sum_t |a_t|^2$),\n",
    "\n",
    "which makes it possible to compare different policies in terms of **stability** and **robustness** to perturbations.\n",
    "\n",
    "[https://energy-locomotion.github.io/resources/CoRL-Energy-Locomotion.pdf](https://energy-locomotion.github.io/resources/CoRL-Energy-Locomotion.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cdf28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_target_speed(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    v_target: float = 1.5,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1e-3,\n",
    "    w_survive: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 2 : vitesse cible + coût d'énergie + survie.\n",
    "\n",
    "        r_t = - alpha * |v_x - v_target|\n",
    "              - beta  * ||a_t||^2\n",
    "              + w_survive * reward_survive\n",
    "\n",
    "    - v_x : vitesse en x du torse (obs[8] dans Walker2d-v5)\n",
    "    - a_t : action au temps t\n",
    "    \"\"\"\n",
    "\n",
    "    # Vitesse horizontale du torse.\n",
    "    # D'après la doc Walker2d-v5, obs[8] = vitesse en x du torse.\n",
    "    vx = float(obs[8])\n",
    "\n",
    "    # Norme au carré de l'action (énergie \"dépensée\")\n",
    "    energy = float(np.sum(np.square(action)))\n",
    "\n",
    "    # Terme de survie fourni par l'env (comme pour reward_speed_energy)\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    # Terme de vitesse : on veut que v_x soit proche de v_target\n",
    "    speed_term = -alpha * abs(vx - v_target)\n",
    "\n",
    "    # Terme d'énergie (pénalisation)\n",
    "    energy_term = -beta * energy\n",
    "\n",
    "    # Récompense totale\n",
    "    reward = speed_term + energy_term + w_survive * survive\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd678e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\miniconda3\\envs\\walker2d\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\arthu\\OneDrive\\Documents\\UTC\\Poly mtl\\RL\\Projet\\videos_target_speed folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.2     |\n",
      "|    ep_rew_mean     | -36.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 109      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.27    |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.998    |\n",
      "|    ent_coef_loss   | -0.0208  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8        |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.8     |\n",
      "|    ep_rew_mean     | -37.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 222      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.85    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.964    |\n",
      "|    ent_coef_loss   | -0.365   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 121      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.3     |\n",
      "|    ep_rew_mean     | -34.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 496      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.07    |\n",
      "|    critic_loss     | 0.523    |\n",
      "|    ent_coef        | 0.888    |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 395      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56.2     |\n",
      "|    ep_rew_mean     | -60.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 900      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.3    |\n",
      "|    critic_loss     | 0.63     |\n",
      "|    ent_coef        | 0.788    |\n",
      "|    ent_coef_loss   | -2.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67       |\n",
      "|    ep_rew_mean     | -80.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 1340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.8    |\n",
      "|    critic_loss     | 0.667    |\n",
      "|    ent_coef        | 0.693    |\n",
      "|    ent_coef_loss   | -3.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.5     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 2028     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.6    |\n",
      "|    critic_loss     | 0.762    |\n",
      "|    ent_coef        | 0.569    |\n",
      "|    ent_coef_loss   | -4.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1927     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -97.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 2820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.454    |\n",
      "|    ent_coef_loss   | -6.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 3640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.358    |\n",
      "|    ent_coef_loss   | -8.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 121      |\n",
      "|    ep_rew_mean     | -90.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 4370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.29     |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 132      |\n",
      "|    ep_rew_mean     | -89      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 5285     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5184     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 139      |\n",
      "|    ep_rew_mean     | -82.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 6121     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | -75.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 6903     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.143    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6802     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | -82.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 7855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 4.43     |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7754     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -81.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 8763     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 3.58     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8662     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | -74.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 9396     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.1    |\n",
      "|    critic_loss     | 3.98     |\n",
      "|    ent_coef        | 0.0736   |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9295     |\n",
      "---------------------------------\n",
      "Épisode terminé. Vidéo enregistrée dans : ./videos_target_speed\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- Wrapper pour la récompense \"vitesse cible\" ---\n",
    "\n",
    "class TargetSpeedRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, v_target=1.5, alpha=1.0, beta=1e-3, w_survive=1.0):\n",
    "        super().__init__(env)\n",
    "        self.v_target = v_target\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.w_survive = w_survive\n",
    "\n",
    "    def step(self, action):\n",
    "        # Env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Notre nouvelle récompense\n",
    "        new_reward = reward_target_speed(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            v_target=self.v_target,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "            w_survive=self.w_survive,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --- Création de l'env + enregistrement vidéo ---\n",
    "\n",
    "video_folder = \"./videos_target_speed\"\n",
    "\n",
    "env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "env_wrapped = TargetSpeedRewardWrapper(\n",
    "    env_base,\n",
    "    v_target=1.5,   # vitesse cible en m/s (tu peux tester 1.0, 2.0, etc.)\n",
    "    alpha=1.0,\n",
    "    beta=1e-3,\n",
    "    w_survive=1.0,\n",
    ")\n",
    "\n",
    "env = RecordVideo(\n",
    "    env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-target_speed\",\n",
    "    episode_trigger=lambda ep_id: True,\n",
    "    video_length=0,\n",
    ")\n",
    "\n",
    "# --- Entraînement rapide avec SAC (juste pour tester) ---\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# --- On filme un épisode avec le modèle entraîné ---\n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "print(f\"Épisode terminé. Vidéo enregistrée dans : {video_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f2350",
   "metadata": {},
   "source": [
    "### Reward function 3: stable posture\n",
    "\n",
    "This reward aims to encourage not only forward movement but also a **stable posture** — walking “upright and straight.”\n",
    "\n",
    "In `Walker2d-v5`:\n",
    "\n",
    "* `info[\"reward_forward\"]`: forward speed term,\n",
    "* `info[\"reward_ctrl\"]`: control cost (negative, penalizes large actions),\n",
    "* `info[\"reward_survive\"]`: survival bonus when the robot stays in a “healthy” state,\n",
    "* `obs[0]`: torso height,\n",
    "* `obs[1]`: torso angle (0 = vertical).\n",
    "\n",
    "The reward is defined as:\n",
    "\n",
    "$ r_t = w_{\\text{forward}} \\cdot \\text{reward\\_forward} + w_{\\text{ctrl}} \\cdot \\text{reward\\_ctrl} + w_{\\text{survive}} \\cdot \\text{reward\\_survive} - w_h \\, \\max(0, h_{\\text{target}} - h_t)^2 - w_{\\text{angle}} \\, \\theta_t^2 $\n",
    "\n",
    "where:\n",
    "\n",
    "* $h_t$ is the torso height at time $t$,\n",
    "* $\\theta_t$ is the torso angle (0 = straight),\n",
    "* $h_{\\text{target}}$ is a target height (e.g., 1.25),\n",
    "* $w_h$ controls the importance of staying high enough,\n",
    "* $w_{\\text{angle}}$ controls the importance of staying upright.\n",
    "\n",
    "#### Motivation and interest with noise / random resets\n",
    "\n",
    "With observation noise and random initial conditions, the walker tends to:\n",
    "\n",
    "* lean forward or backward too much,\n",
    "* sag (drop torso height) before falling.\n",
    "\n",
    "This “stable posture” reward:\n",
    "\n",
    "* gradually penalizes **dangerous postures** (too tilted, too low),\n",
    "* provides a reward signal **before falling**,\n",
    "* encourages more **stable** gaits, which are often more **robust** to perturbations.\n",
    "\n",
    "We can compare this reward to others by measuring:\n",
    "\n",
    "* the average torso height,\n",
    "* the variance of the torso angle,\n",
    "* the number of falls / terminations per episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d047b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def reward_posture_stability(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    h_target: float = 1.25,\n",
    "    w_forward: float = 1.0,\n",
    "    w_ctrl: float = 1.0,\n",
    "    w_survive: float = 1.0,\n",
    "    w_h: float = 1.0,\n",
    "    w_angle: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 3 : posture stable\n",
    "\n",
    "    Idée :\n",
    "      - garder les termes classiques (vitesse, coût de contrôle, survie)\n",
    "      - ajouter des termes qui encouragent une posture \"debout et droite\"\n",
    "\n",
    "    r_t = w_forward * reward_forward\n",
    "        + w_ctrl    * reward_ctrl\n",
    "        + w_survive * reward_survive\n",
    "        + height_term\n",
    "        + angle_term\n",
    "\n",
    "    où :\n",
    "      - height_term pénalise si la hauteur du torse est en dessous d'une cible h_target\n",
    "      - angle_term pénalise si l'angle du torse s'éloigne de 0\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Termes classiques de Walker2d ---\n",
    "\n",
    "    forward = float(info.get(\"reward_forward\", 0.0))\n",
    "    ctrl = float(info.get(\"reward_ctrl\", 0.0))         # déjà négatif\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    base_terms = (\n",
    "        w_forward * forward\n",
    "        + w_ctrl * ctrl\n",
    "        + w_survive * survive\n",
    "    )\n",
    "\n",
    "    # --- 2) Termes de posture ---\n",
    "\n",
    "    # D'après la doc Walker2d-v5 :\n",
    "    #   obs[0] = hauteur du torse\n",
    "    #   obs[1] = angle du torse (0 = droit).  :contentReference[oaicite:0]{index=0}\n",
    "    h = float(obs[0])\n",
    "    angle = float(obs[1])\n",
    "\n",
    "    # pénalité si le torse est plus bas que h_target (quadratique)\n",
    "    height_penalty = -w_h * max(0.0, h_target - h) ** 2\n",
    "\n",
    "    # pénalité quadratique sur l'angle (on veut angle ≈ 0)\n",
    "    angle_penalty = -w_angle * angle ** 2\n",
    "\n",
    "    reward = base_terms + height_penalty + angle_penalty\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49b88d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | -5.22    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2286     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 74       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -5.74    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 229      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 157      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.55    |\n",
      "|    critic_loss     | 3.52     |\n",
      "|    ent_coef        | 0.984    |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 56       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -5.27    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 229      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.34    |\n",
      "|    critic_loss     | 3.11     |\n",
      "|    ent_coef        | 0.962    |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 128      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -5.75    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 313      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.25    |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.938    |\n",
      "|    ent_coef_loss   | -0.638   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 212      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | -6.49    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 109      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 446      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.8    |\n",
      "|    critic_loss     | 0.957    |\n",
      "|    ent_coef        | 0.902    |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 345      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | -7.31    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 578      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12      |\n",
      "|    critic_loss     | 0.827    |\n",
      "|    ent_coef        | 0.867    |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 477      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.4     |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 767      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13      |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.82     |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 666      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.2     |\n",
      "|    ep_rew_mean     | -10      |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1061     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.752    |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 960      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43       |\n",
      "|    ep_rew_mean     | -13.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1547     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.655    |\n",
      "|    ent_coef_loss   | -3.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1446     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.6     |\n",
      "|    ep_rew_mean     | -15      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 2183     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.9    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.548    |\n",
      "|    ent_coef_loss   | -5.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2082     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.8     |\n",
      "|    ep_rew_mean     | 4.33     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 2938     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.2    |\n",
      "|    critic_loss     | 5.13     |\n",
      "|    ent_coef        | 0.443    |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2837     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 4241     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.2    |\n",
      "|    critic_loss     | 6.39     |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | -9.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.3     |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 4749     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.5    |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.264    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4648     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 5348     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.8    |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5247     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 29.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 6034     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -40.3    |\n",
      "|    critic_loss     | 5.39     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5933     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 42.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 7081     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.1    |\n",
      "|    critic_loss     | 9.7      |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 119      |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 8082     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.8    |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -9.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7981     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 76       |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 9143     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.5    |\n",
      "|    critic_loss     | 9.24     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -8.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9042     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (pas de vidéo ici) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # PAS de render_mode pour l'entraînement\n",
    "\n",
    "train_env = PostureStabilityRewardWrapper(\n",
    "    train_env_base,\n",
    "    h_target=1.25,\n",
    "    w_forward=1.0,\n",
    "    w_ctrl=1.0,\n",
    "    w_survive=1.0,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "model = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# (optionnel) sauver le modèle\n",
    "model.save(\"sac_walker2d_posture_stable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c713aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\miniconda3\\envs\\walker2d\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\arthu\\OneDrive\\Documents\\UTC\\Poly mtl\\RL\\Projet\\videos_posture_stable folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_posture_stable\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "video_folder = \"./videos_posture_stable\"\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = PostureStabilityRewardWrapper(\n",
    "    eval_env_base,\n",
    "    h_target=1.25,\n",
    "    w_forward=1.0,\n",
    "    w_ctrl=1.0,\n",
    "    w_survive=1.0,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-posture_stable\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le premier épisode\n",
    "    video_length=0,                       # 0 = épisode complet\n",
    ")\n",
    "\n",
    "# si tu as sauvegardé le modèle :\n",
    "# model = SAC.load(\"sac_walker2d_posture_stable\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c000a5b",
   "metadata": {},
   "source": [
    "### Reward function 4: smooth actions\n",
    "\n",
    "This reward aims to avoid policies that react **too nervously** to noise (noisy observations, random resets, etc.).\n",
    "The idea is to penalize **sudden changes in actions** between two time steps:\n",
    "\n",
    "$ r_t = \\text{base\\_reward}_t - \\lambda \\, \\lVert a_t - a_{t-1} \\rVert^2, $\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\text{base\\_reward}_t$ is the original reward from the `Walker2d-v5` environment (speed + survival − control cost),\n",
    "* $a_t$ is the action at time $t$,\n",
    "* $a_{t-1}$ is the action at time $t-1$,\n",
    "* $\\lambda$ (called `lambda_smooth` in the code) controls how strongly action changes are penalized.\n",
    "\n",
    "#### Motivation and interest with noise\n",
    "\n",
    "With observation noise, RL algorithms tend to:\n",
    "\n",
    "* react strongly to small fluctuations,\n",
    "* produce actions that oscillate very quickly,\n",
    "* show less stable gaits and sometimes higher energy usage.\n",
    "\n",
    "The “smooth actions” reward encourages:\n",
    "\n",
    "* **more regular control signals** (fewer high-frequency oscillations),\n",
    "* gaits that are often more **stable** and more **robust** to noise,\n",
    "* a more “conservative” policy in its action changes.\n",
    "\n",
    "We can compare different values of $\\lambda$ (e.g., `1e-3`, `1e-2`, `5e-2`) and measure:\n",
    "\n",
    "* the variance of the actions,\n",
    "* energy consumption,\n",
    "* robustness to perturbations.\n",
    "\n",
    "[https://medium.com/correll-lab/towards-robust-humanoid-loco-manipulation-using-deep-reinforcement-learning-45c8a5a0fcbf](https://medium.com/correll-lab/towards-robust-humanoid-loco-manipulation-using-deep-reinforcement-learning-45c8a5a0fcbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6dcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_smooth_actions(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    base_reward: float,\n",
    "    prev_action,\n",
    "    lambda_smooth: float = 1e-2,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 4 : actions lisses (anti-réaction nerveuse au bruit)\n",
    "\n",
    "    r_t = base_reward_t - lambda_smooth * ||a_t - a_{t-1}||^2\n",
    "\n",
    "    - base_reward_t : récompense originale de l'environnement Walker2d-v5\n",
    "    - a_t           : action actuelle\n",
    "    - a_{t-1}       : action précédente\n",
    "    \"\"\"\n",
    "    # Si on n'a pas encore d'action précédente (premier step), on ne pénalise pas\n",
    "    if prev_action is None:\n",
    "        return float(base_reward)\n",
    "\n",
    "    # Différence entre action actuelle et précédente\n",
    "    delta = action - prev_action\n",
    "\n",
    "    # Norme au carré de la différence (grands changements d'actions = pénalisés)\n",
    "    smooth_penalty = lambda_smooth * float(np.sum(delta ** 2))\n",
    "\n",
    "    new_reward = float(base_reward) - smooth_penalty\n",
    "    return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ef5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class SmoothActionsRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, lambda_smooth: float = 1e-2):\n",
    "        super().__init__(env)\n",
    "        self.lambda_smooth = lambda_smooth\n",
    "        self.prev_action = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Au reset, on n'a pas encore d'action précédente\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Appel de l'env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Calcul de notre récompense \"actions lisses\"\n",
    "        new_reward = reward_smooth_actions(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            base_reward=base_reward,\n",
    "            prev_action=self.prev_action,\n",
    "            lambda_smooth=self.lambda_smooth,\n",
    "        )\n",
    "\n",
    "        # Mise à jour de l'action précédente\n",
    "        self.prev_action = np.array(action, copy=True)\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f13e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -2.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 1648     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 77       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -2.29    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 234      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 153      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.68    |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.985    |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.7     |\n",
      "|    ep_rew_mean     | -1.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 212      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.55    |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.967    |\n",
      "|    ent_coef_loss   | -0.334   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 111      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -0.97    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 108      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 315      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.69    |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.938    |\n",
      "|    ent_coef_loss   | -0.642   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 214      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -0.271   |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 455      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.2    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.899    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 354      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.3     |\n",
      "|    ep_rew_mean     | 0.349    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 583      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.1    |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 482      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.8     |\n",
      "|    ep_rew_mean     | 0.169    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 863      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15      |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.797    |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 762      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.6     |\n",
      "|    ep_rew_mean     | -2.53    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1234     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.3    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.715    |\n",
      "|    ent_coef_loss   | -3.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1133     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45.4     |\n",
      "|    ep_rew_mean     | -4.49    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1634     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.5    |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.637    |\n",
      "|    ent_coef_loss   | -4.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1533     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58       |\n",
      "|    ep_rew_mean     | 3.42     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 2321     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.9    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.526    |\n",
      "|    ent_coef_loss   | -5.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2220     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.5     |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 3410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.3    |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.388    |\n",
      "|    ent_coef_loss   | -7.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.1     |\n",
      "|    ep_rew_mean     | 47.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 4371     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.3    |\n",
      "|    critic_loss     | 3.08     |\n",
      "|    ent_coef        | 0.294    |\n",
      "|    ent_coef_loss   | -9.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4270     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.5     |\n",
      "|    ep_rew_mean     | 67.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 5176     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.1    |\n",
      "|    critic_loss     | 4.72     |\n",
      "|    ent_coef        | 0.234    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5075     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | 82.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 5837     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.4    |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5736     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 94.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 6488     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.8    |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.164    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6387     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 7121     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.6    |\n",
      "|    critic_loss     | 17.2     |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | 115      |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 7706     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.7    |\n",
      "|    critic_loss     | 8.43     |\n",
      "|    ent_coef        | 0.12     |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7605     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | 118      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 8469     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.8    |\n",
      "|    critic_loss     | 8.53     |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -9       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8368     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 9097     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -67.3    |\n",
      "|    critic_loss     | 7.85     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | -9.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8996     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 9917     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.1    |\n",
      "|    critic_loss     | 7.97     |\n",
      "|    ent_coef        | 0.0722   |\n",
      "|    ent_coef_loss   | -5.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9816     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x2963cc2e2f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (sans vidéo) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # pas de render_mode ici\n",
    "train_env = SmoothActionsRewardWrapper(\n",
    "    train_env_base,\n",
    "    lambda_smooth=1e-2,  # tu peux tester différentes valeurs\n",
    ")\n",
    "\n",
    "model_smooth = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_smooth.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# optionnel : sauver le modèle\n",
    "# model_smooth.save(\"sac_walker2d_smooth_actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a09515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_smooth_actions\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_smooth_actions\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "eval_env_wrapped = SmoothActionsRewardWrapper(\n",
    "    eval_env_base,\n",
    "    lambda_smooth=1e-2,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-smooth_actions\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le premier épisode\n",
    "    video_length=0,                       # épisode complet\n",
    ")\n",
    "\n",
    "# Si tu avais sauvegardé : model_smooth = SAC.load(\"sac_walker2d_smooth_actions\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_smooth.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6e4de",
   "metadata": {},
   "source": [
    "### Reward function 5: dynamic stability (penalizing state jolts)\n",
    "\n",
    "This reward aims to make the walking not only “good” at a given moment, but also **stable over time**, by penalizing changes in the robot’s state that are too abrupt.\n",
    "\n",
    "We define:\n",
    "\n",
    "* $\\mathbf{s}_t$: the state / observation vector at time $t$ (in `Walker2d-v5`, this includes for example torso height, angle, velocities, etc.),\n",
    "* $\\mathbf{a}_t$: the action at time $t$,\n",
    "* $\\text{base\\_reward}_t$: the environment’s base reward (speed, survival, control cost).\n",
    "\n",
    "We approximate a **state acceleration** using a discrete second derivative:\n",
    "\n",
    "$ \\mathbf{a}^{(\\text{state})}_t \\approx \\mathbf{s}_t - 2 \\mathbf{s}_{t-1} + \\mathbf{s}_{t-2}, $\n",
    "\n",
    "and we define:\n",
    "\n",
    "$ r_t = \\text{base\\_reward}_t - \\lambda_{\\text{state}} \\, \\left\\|\\mathbf{a}^{(\\text{state})}_t\\right\\|^2, $\n",
    "\n",
    "where $\\lambda_{\\text{state}} > 0$ controls the strength of the penalty.\n",
    "The more the state “accelerates” (rapid changes in posture or velocity), the larger the penalty.\n",
    "\n",
    "#### Interest under noise and perturbations\n",
    "\n",
    "With observation noise or random resets, learned policies can become:\n",
    "\n",
    "* very **reactive** to small variations in the state,\n",
    "* producing jerky movements,\n",
    "* less **dynamically stable**.\n",
    "\n",
    "By penalizing **state accelerations**, we encourage trajectories that are:\n",
    "\n",
    "* **smoother** over time,\n",
    "* with more regular posture changes,\n",
    "* often more **robust** to perturbations.\n",
    "\n",
    "This idea is related to many works that add terms on joint velocities / accelerations or on trunk dynamics to produce more stable and natural walking in RL locomotion tasks.\n",
    "\n",
    "[https://medium.com/correll-lab/towards-robust-humanoid-loco-manipulation-using-deep-reinforcement-learning-45c8a5a0fcbf](https://medium.com/correll-lab/towards-robust-humanoid-loco-manipulation-using-deep-reinforcement-learning-45c8a5a0fcbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc54d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_dynamic_stability(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    base_reward: float,\n",
    "    prev_obs,\n",
    "    prev_prev_obs,\n",
    "    lambda_state: float = 1e-2,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 5 : stabilité dynamique\n",
    "\n",
    "    Idée :\n",
    "      - on part de la récompense de base de l'environnement (base_reward)\n",
    "      - on pénalise les \"secousses\" de l'état, c-à-d les fortes accélérations\n",
    "        entre trois pas de temps consécutifs.\n",
    "\n",
    "    On approxime une dérivée seconde de l'état par :\n",
    "        accel ≈ obs_t - 2 * obs_{t-1} + obs_{t-2}\n",
    "\n",
    "    Puis :\n",
    "        r_t = base_reward - lambda_state * ||accel||^2\n",
    "    \"\"\"\n",
    "\n",
    "    # Si on n'a pas encore assez d'historique (au début de l'épisode),\n",
    "    # on ne pénalise pas.\n",
    "    if prev_obs is None or prev_prev_obs is None:\n",
    "        return float(base_reward)\n",
    "\n",
    "    # On calcule une \"accélération\" approximative sur tout le vecteur d'observation\n",
    "    accel = obs - 2.0 * prev_obs + prev_prev_obs\n",
    "\n",
    "    # Norme au carré des accélérations de l'état\n",
    "    accel_penalty = lambda_state * float(np.sum(accel ** 2))\n",
    "\n",
    "    new_reward = float(base_reward) - accel_penalty\n",
    "    return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a501eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class DynamicStabilityRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, lambda_state: float = 1e-2):\n",
    "        super().__init__(env)\n",
    "        self.lambda_state = lambda_state\n",
    "        self.prev_obs = None\n",
    "        self.prev_prev_obs = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Au début de l'épisode, on n'a pas encore d'historique\n",
    "        self.prev_obs = None\n",
    "        self.prev_prev_obs = None\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # step de l'env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Calcul de notre récompense \"stabilité dynamique\"\n",
    "        new_reward = reward_dynamic_stability(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            base_reward=base_reward,\n",
    "            prev_obs=self.prev_obs,\n",
    "            prev_prev_obs=self.prev_prev_obs,\n",
    "            lambda_state=self.lambda_state,\n",
    "        )\n",
    "\n",
    "        # Mise à jour de l'historique des observations\n",
    "        self.prev_prev_obs = self.prev_obs\n",
    "        self.prev_obs = np.array(obs, copy=True)\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f62750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | -87.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 520      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 116      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.93    |\n",
      "|    critic_loss     | 9.55     |\n",
      "|    ent_coef        | 0.996    |\n",
      "|    ent_coef_loss   | -0.0421  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27       |\n",
      "|    ep_rew_mean     | -75      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 216      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.72    |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.966    |\n",
      "|    ent_coef_loss   | -0.346   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 115      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.2     |\n",
      "|    ep_rew_mean     | -71.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 314      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.41    |\n",
      "|    critic_loss     | 6.08     |\n",
      "|    ent_coef        | 0.938    |\n",
      "|    ent_coef_loss   | -0.631   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 213      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.8     |\n",
      "|    ep_rew_mean     | -65.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 413      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.69    |\n",
      "|    critic_loss     | 5.87     |\n",
      "|    ent_coef        | 0.911    |\n",
      "|    ent_coef_loss   | -0.915   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 312      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.7     |\n",
      "|    ep_rew_mean     | -57.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 494      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.51    |\n",
      "|    critic_loss     | 3.83     |\n",
      "|    ent_coef        | 0.889    |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 393      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -51.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 572      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.02    |\n",
      "|    critic_loss     | 3.67     |\n",
      "|    ent_coef        | 0.869    |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 471      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -48      |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 648      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.15    |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.85     |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 547      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.7     |\n",
      "|    ep_rew_mean     | -46.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 758      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.76    |\n",
      "|    critic_loss     | 3.07     |\n",
      "|    ent_coef        | 0.823    |\n",
      "|    ent_coef_loss   | -1.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 657      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.6     |\n",
      "|    ep_rew_mean     | -46.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 848      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.59    |\n",
      "|    critic_loss     | 2.95     |\n",
      "|    ent_coef        | 0.802    |\n",
      "|    ent_coef_loss   | -2.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 747      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -46.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 978      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.32    |\n",
      "|    critic_loss     | 3.32     |\n",
      "|    ent_coef        | 0.773    |\n",
      "|    ent_coef_loss   | -2.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 877      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -43.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 1073     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.33    |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.752    |\n",
      "|    ent_coef_loss   | -2.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 972      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.2     |\n",
      "|    ep_rew_mean     | -43.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 1208     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.12    |\n",
      "|    critic_loss     | 2.87     |\n",
      "|    ent_coef        | 0.723    |\n",
      "|    ent_coef_loss   | -2.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1107     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.7     |\n",
      "|    ep_rew_mean     | -45.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 1439     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.5     |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.678    |\n",
      "|    ent_coef_loss   | -3.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1338     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | -48.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 1702     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.88    |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.629    |\n",
      "|    ent_coef_loss   | -3.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | -54.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 2186     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.59    |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.55     |\n",
      "|    ent_coef_loss   | -4.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2085     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.6     |\n",
      "|    ep_rew_mean     | -59.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 2665     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.54    |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.481    |\n",
      "|    ent_coef_loss   | -5.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2564     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.1     |\n",
      "|    ep_rew_mean     | -63.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.87    |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.414    |\n",
      "|    ent_coef_loss   | -6.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 61       |\n",
      "|    ep_rew_mean     | -57      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 4392     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.83    |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.297    |\n",
      "|    ent_coef_loss   | -8.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4291     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67.5     |\n",
      "|    ep_rew_mean     | -41.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 5129     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.3    |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.243    |\n",
      "|    ent_coef_loss   | -8.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5028     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.5     |\n",
      "|    ep_rew_mean     | -26.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 5884     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13      |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.2      |\n",
      "|    ent_coef_loss   | -9.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5783     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.7     |\n",
      "|    ep_rew_mean     | -13.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 6692     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.3    |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | -7.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6591     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.5     |\n",
      "|    ep_rew_mean     | -0.792   |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 7350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22      |\n",
      "|    critic_loss     | 5.73     |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -7.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7249     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.2     |\n",
      "|    ep_rew_mean     | 12.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 8117     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.6    |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    ent_coef        | 0.119    |\n",
      "|    ent_coef_loss   | -6.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8016     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.9     |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 8918     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.9    |\n",
      "|    critic_loss     | 6.42     |\n",
      "|    ent_coef        | 0.0991   |\n",
      "|    ent_coef_loss   | -5.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8817     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.3     |\n",
      "|    ep_rew_mean     | 32.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 9627     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32.6    |\n",
      "|    critic_loss     | 4.26     |\n",
      "|    ent_coef        | 0.0844   |\n",
      "|    ent_coef_loss   | -4.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9526     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x23fff996110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (sans vidéo) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # pas de render_mode ici\n",
    "\n",
    "train_env = DynamicStabilityRewardWrapper(\n",
    "    train_env_base,\n",
    "    lambda_state=1e-2,   # tu peux tester 1e-3, 1e-2, 5e-2, etc.\n",
    ")\n",
    "\n",
    "model_dyn_stab = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_dyn_stab.learn(total_timesteps=10_000)  # à augmenter plus tard\n",
    "\n",
    "# optionnel :\n",
    "# model_dyn_stab.save(\"sac_walker2d_dynamic_stability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06641d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_dynamic_stability\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_dynamic_stability\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = DynamicStabilityRewardWrapper(\n",
    "    eval_env_base,\n",
    "    lambda_state=1e-2,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-dynamic_stability\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le 1er épisode\n",
    "    video_length=0,                       # épisode complet\n",
    ")\n",
    "\n",
    "# si tu as sauvegardé :\n",
    "# model_dyn_stab = SAC.load(\"sac_walker2d_dynamic_stability\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_dyn_stab.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c0f49",
   "metadata": {},
   "source": [
    "### Reward function 6: progressive anti-fall (shaping around dangerous states)\n",
    "\n",
    "This reward aims to help the walker **avoid falling** by progressively penalizing “dangerous” states, instead of giving only a large penalty when the episode ends.\n",
    "\n",
    "In `Walker2d-v5`, we use in particular:\n",
    "\n",
    "* `base_reward`: the environment’s base reward (forward speed, survival, control cost)\n",
    "* `obs[0]`: torso height\n",
    "* `obs[1]`: torso angle (0 = vertical torso)\n",
    "\n",
    "We introduce two thresholds:\n",
    "\n",
    "* $h_{\\text{crit}}$: critical height below which the robot is considered too low (close to collapsing)\n",
    "* $\\theta_{\\text{crit}}$: critical angle above which the torso is too tilted\n",
    "\n",
    "The reward is defined as:\n",
    "\n",
    "$ r_t = \\text{base\\_reward}_t - w_h \\, \\max(0, h_{\\text{crit}} - h_t)^2 - w_{\\text{angle}} \\, \\max(0, |\\theta_t| - \\theta_{\\text{crit}})^2 $\n",
    "\n",
    "where:\n",
    "\n",
    "* $h_t$ is the torso height at time $t$,\n",
    "* $\\theta_t$ is the torso angle,\n",
    "* $w_h$ controls the importance of penalizing dangerous height,\n",
    "* $w_{\\text{angle}}$ controls the importance of penalizing dangerous angle.\n",
    "\n",
    "#### Interest of this shaping\n",
    "\n",
    "* States **close to falling** (too low, too tilted) are penalized before the end of the episode.\n",
    "* The agent therefore receives a **progressive danger signal**, enabling it to learn to:\n",
    "\n",
    "  * straighten up,\n",
    "  * avoid sagging,\n",
    "  * correct its posture instead of simply falling.\n",
    "\n",
    "In a context of **noise** (noisy observations, random resets), this reward helps test whether RL algorithms learn more **robust** behaviors by reducing:\n",
    "\n",
    "* the number of falls per episode,\n",
    "* the time spent in dangerous postures (height < $h_{\\text{crit}}$, angle > $\\theta_{\\text{crit}}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25238e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_anti_fall_progressive(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    base_reward: float,\n",
    "    h_crit: float = 0.9,\n",
    "    angle_crit: float = 0.5,\n",
    "    w_h: float = 5.0,\n",
    "    w_angle: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 6 : anti-chute progressive (shaping autour des états dangereux)\n",
    "\n",
    "    Idée :\n",
    "      - on part de la récompense de base de l'environnement (base_reward)\n",
    "      - on enlève un bonus quand le walker se rapproche de la chute :\n",
    "          * torse trop bas  (h < h_crit)\n",
    "          * torse trop penché (|angle| > angle_crit)\n",
    "\n",
    "    r_t = base_reward\n",
    "          - w_h     * max(0, h_crit - h_t)^2\n",
    "          - w_angle * max(0, |angle_t| - angle_crit)^2\n",
    "    \"\"\"\n",
    "\n",
    "    # D'après Walker2d-v5 :\n",
    "    #   obs[0] = hauteur du torse\n",
    "    #   obs[1] = angle du torse (0 = vertical)\n",
    "    h = float(obs[0])\n",
    "    angle = float(obs[1])\n",
    "\n",
    "    # Danger de hauteur : si le torse descend sous h_crit\n",
    "    height_danger = max(0.0, h_crit - h)\n",
    "    height_penalty = w_h * height_danger**2\n",
    "\n",
    "    # Danger d'angle : si |angle| dépasse angle_crit\n",
    "    angle_danger = max(0.0, abs(angle) - angle_crit)\n",
    "    angle_penalty = w_angle * angle_danger**2\n",
    "\n",
    "    new_reward = float(base_reward) - height_penalty - angle_penalty\n",
    "    return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf3ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class AntiFallProgressiveRewardWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        h_crit: float = 0.9,\n",
    "        angle_crit: float = 0.5,\n",
    "        w_h: float = 5.0,\n",
    "        w_angle: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.h_crit = h_crit\n",
    "        self.angle_crit = angle_crit\n",
    "        self.w_h = w_h\n",
    "        self.w_angle = w_angle\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        # step de l'env original\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # récompense 6 : anti-chute progressive\n",
    "        new_reward = reward_anti_fall_progressive(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            base_reward=base_reward,\n",
    "            h_crit=self.h_crit,\n",
    "            angle_crit=self.angle_crit,\n",
    "            w_h=self.w_h,\n",
    "            w_angle=self.w_angle,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aafcdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.2     |\n",
      "|    ep_rew_mean     | 0.436    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 1964     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 65       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.5     |\n",
      "|    ep_rew_mean     | 1.11     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 269      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 132      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.24    |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.991    |\n",
      "|    ent_coef_loss   | -0.0907  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.4     |\n",
      "|    ep_rew_mean     | 0.182    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 197      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.34    |\n",
      "|    critic_loss     | 3.01     |\n",
      "|    ent_coef        | 0.972    |\n",
      "|    ent_coef_loss   | -0.291   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 96       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | 4        |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 392      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.6    |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.916    |\n",
      "|    ent_coef_loss   | -0.869   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 291      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.7     |\n",
      "|    ep_rew_mean     | 4.91     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 634      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.7    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.853    |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 533      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.6     |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 80       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 927      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 0.978    |\n",
      "|    ent_coef        | 0.782    |\n",
      "|    ent_coef_loss   | -2.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 826      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.3     |\n",
      "|    ep_rew_mean     | 0.0992   |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1408     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.681    |\n",
      "|    ent_coef_loss   | -3.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1307     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2362     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.6    |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.519    |\n",
      "|    ent_coef_loss   | -5.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2261     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.5     |\n",
      "|    ep_rew_mean     | 54.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 3079     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.3    |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.424    |\n",
      "|    ent_coef_loss   | -6.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2978     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 84.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 4315     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.2    |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.3      |\n",
      "|    ent_coef_loss   | -9.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4214     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 116      |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 5969     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.4    |\n",
      "|    critic_loss     | 4.04     |\n",
      "|    ent_coef        | 0.189    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5868     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | 119      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 6856     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.3    |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6755     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 147      |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 7628     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.6    |\n",
      "|    critic_loss     | 7.74     |\n",
      "|    ent_coef        | 0.121    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7527     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 8451     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.4    |\n",
      "|    critic_loss     | 8.09     |\n",
      "|    ent_coef        | 0.0982   |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8350     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.6    |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0743   |\n",
      "|    ent_coef_loss   | -7.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1f5e77c3a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (pas de vidéo ici) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # pas de render_mode\n",
    "\n",
    "train_env = AntiFallProgressiveRewardWrapper(\n",
    "    train_env_base,\n",
    "    h_crit=0.9,\n",
    "    angle_crit=0.5,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "model_anti_fall = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_anti_fall.learn(total_timesteps=10_000)  # à augmenter plus tard\n",
    "\n",
    "# optionnel :\n",
    "# model_anti_fall.save(\"sac_walker2d_anti_fall_progressive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235ff863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\miniconda3\\envs\\walker2d\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\arthu\\OneDrive\\Documents\\UTC\\Poly mtl\\RL\\Projet\\videos_anti_fall_progressive folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_anti_fall_progressive\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_anti_fall_progressive\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = AntiFallProgressiveRewardWrapper(\n",
    "    eval_env_base,\n",
    "    h_crit=0.9,\n",
    "    angle_crit=0.5,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-anti_fall\",\n",
    "    episode_trigger=lambda ep_id: True,\n",
    "    video_length=0,  # épisode complet\n",
    ")\n",
    "\n",
    "# si tu avais sauvegardé :\n",
    "# model_anti_fall = SAC.load(\"sac_walker2d_anti_fall_progressive\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_anti_fall.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf83f60",
   "metadata": {},
   "source": [
    "### Reward function 7: robust & economical walking (simple multi-objective)\n",
    "\n",
    "This reward explicitly combines three objectives:\n",
    "\n",
    "1. **Moving forward** (speed),\n",
    "2. **Using little energy** (moderate actions),\n",
    "3. **Staying high enough** (overall stable posture).\n",
    "\n",
    "In `Walker2d-v5`:\n",
    "\n",
    "* `obs[8]` corresponds to the torso’s x-velocity,\n",
    "* `obs[0]` corresponds to the torso height,\n",
    "* the energy used can be approximated by $|a_t|^2$ (squared norm of the action).\n",
    "\n",
    "We define:\n",
    "\n",
    "$ r_t = w_v \\, v_x - w_E \\, \\|a_t\\|^2 + w_h \\, \\max(0, h_t - h_{\\min}) + w_{\\text{survive}} \\cdot \\text{reward\\_survive} $\n",
    "\n",
    "where:\n",
    "\n",
    "* $v_x$ is the torso’s horizontal speed,\n",
    "* $|a_t|^2$ measures the instantaneous energy of the actions,\n",
    "* $h_t$ is the torso height,\n",
    "* $h_{\\min}$ is a desired minimum height (e.g., 1.0),\n",
    "* $w_v$ (`v_weight`) sets the importance of speed,\n",
    "* $w_E$ (`energy_weight`) sets the importance of energy saving,\n",
    "* $w_h$ (`h_weight`) sets the importance of staying high,\n",
    "* $w_{\\text{survive}}$ (`w_survive`) optionally keeps a survival term.\n",
    "\n",
    "#### Interest for robustness\n",
    "\n",
    "This reward makes it possible to study a clear trade-off between:\n",
    "\n",
    "* **performance** (distance traveled, average speed),\n",
    "* **energy cost** (sum of (|a_t|^2)),\n",
    "* **overall stability** (average torso height).\n",
    "\n",
    "Under perturbations (observation noise, randomized resets), we can compare different policies by looking at:\n",
    "\n",
    "* the distance traveled,\n",
    "* total energy consumed,\n",
    "* average height and number of falls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091fee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_robust_econ(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    v_weight: float = 1.0,\n",
    "    energy_weight: float = 1e-3,\n",
    "    h_weight: float = 1.0,\n",
    "    h_min: float = 1.0,\n",
    "    w_survive: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 7 : marche robuste & économe (multi-objectif simple)\n",
    "\n",
    "    Objectif : combiner trois critères :\n",
    "      - vitesse vers l'avant (vx)\n",
    "      - énergie consommée (||a_t||^2)\n",
    "      - hauteur du torse (h)\n",
    "\n",
    "    r_t = v_weight     * v_x\n",
    "        - energy_weight * ||a_t||^2\n",
    "        + h_weight     * max(0, h_t - h_min)\n",
    "        + w_survive    * reward_survive\n",
    "\n",
    "    où :\n",
    "      - v_x     : vitesse en x du torse (obs[8] dans Walker2d-v5)\n",
    "      - a_t     : action au temps t\n",
    "      - h_t     : hauteur du torse (obs[0])\n",
    "      - h_min   : hauteur minimale désirée\n",
    "    \"\"\"\n",
    "\n",
    "    # Vitesse horizontale (gait plus rapide)\n",
    "    vx = float(obs[8])          # velocity of x-coordinate of torso\n",
    "\n",
    "    # Énergie des actions\n",
    "    energy = float(np.sum(np.square(action)))\n",
    "\n",
    "    # Hauteur du torse\n",
    "    h = float(obs[0])\n",
    "\n",
    "    # Terme de survie de l'env (optionnel)\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    # Terme de hauteur : on récompense si h > h_min\n",
    "    height_term = h_weight * max(0.0, h - h_min)\n",
    "\n",
    "    reward = (\n",
    "        v_weight * vx\n",
    "        - energy_weight * energy\n",
    "        + height_term\n",
    "        + w_survive * survive\n",
    "    )\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class RobustEconRewardWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        v_weight: float = 1.0,\n",
    "        energy_weight: float = 1e-3,\n",
    "        h_weight: float = 0,\n",
    "        h_min: float = 1.0,\n",
    "        w_survive: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.v_weight = v_weight\n",
    "        self.energy_weight = energy_weight\n",
    "        self.h_weight = h_weight\n",
    "        self.h_min = h_min\n",
    "        self.w_survive = w_survive\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        # step de l'env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # récompense 7 : marche robuste & économe\n",
    "        new_reward = reward_robust_econ(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            v_weight=self.v_weight,\n",
    "            energy_weight=self.energy_weight,\n",
    "            h_weight=self.h_weight,\n",
    "            h_min=self.h_min,\n",
    "            w_survive=self.w_survive,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e9c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -17      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2079     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 79       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -14.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 220      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 157      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.11    |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.983    |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 56       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -15.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 285      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.36    |\n",
      "|    critic_loss     | 0.975    |\n",
      "|    ent_coef        | 0.946    |\n",
      "|    ent_coef_loss   | -0.557   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 184      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.8     |\n",
      "|    ep_rew_mean     | -18.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 460      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.87    |\n",
      "|    critic_loss     | 0.789    |\n",
      "|    ent_coef        | 0.898    |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.5     |\n",
      "|    ep_rew_mean     | -26.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 770      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.1    |\n",
      "|    critic_loss     | 0.68     |\n",
      "|    ent_coef        | 0.819    |\n",
      "|    ent_coef_loss   | -1.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.8     |\n",
      "|    ep_rew_mean     | -38.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.5    |\n",
      "|    critic_loss     | 0.499    |\n",
      "|    ent_coef        | 0.729    |\n",
      "|    ent_coef_loss   | -2.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.1     |\n",
      "|    ep_rew_mean     | -47.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 1824     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.604    |\n",
      "|    ent_coef_loss   | -4.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1723     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.8     |\n",
      "|    ep_rew_mean     | -40.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2456     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.8    |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.505    |\n",
      "|    ent_coef_loss   | -5.96    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2355     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.5     |\n",
      "|    ep_rew_mean     | -19.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 3475     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.9    |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.377    |\n",
      "|    ent_coef_loss   | -7.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3374     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | -2.05    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 4326     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.4    |\n",
      "|    critic_loss     | 2.93     |\n",
      "|    ent_coef        | 0.296    |\n",
      "|    ent_coef_loss   | -9.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4225     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | 13.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 5350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.4    |\n",
      "|    critic_loss     | 8.35     |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5249     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 6222     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -42.1    |\n",
      "|    critic_loss     | 4.44     |\n",
      "|    ent_coef        | 0.175    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6121     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 138      |\n",
      "|    ep_rew_mean     | 38.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -44.6    |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 141      |\n",
      "|    ep_rew_mean     | 47.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 7904     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.8    |\n",
      "|    critic_loss     | 5.82     |\n",
      "|    ent_coef        | 0.113    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7803     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 54       |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 8624     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.7    |\n",
      "|    critic_loss     | 6.69     |\n",
      "|    ent_coef        | 0.094    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8523     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 148      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 9478     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.3    |\n",
      "|    critic_loss     | 6.49     |\n",
      "|    ent_coef        | 0.076    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9377     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1f593389660>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (pas de vidéo ici) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")\n",
    "\n",
    "train_env = RobustEconRewardWrapper(\n",
    "    train_env_base,\n",
    "    v_weight=1.0,\n",
    "    energy_weight=1e-3,\n",
    "    h_weight=1.0,\n",
    "    h_min=1.0,\n",
    "    w_survive=0.0,  # tu peux tester 0.0 ou 1.0\n",
    ")\n",
    "\n",
    "model_robust_econ = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_robust_econ.learn(total_timesteps=10_000)  # à augmenter plus tard\n",
    "\n",
    "# optionnel :\n",
    "# model_robust_econ.save(\"sac_walker2d_robust_econ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3647bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_robust_econ\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_robust_econ\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = RobustEconRewardWrapper(\n",
    "    eval_env_base,\n",
    "    v_weight=1.0,\n",
    "    energy_weight=1e-3,\n",
    "    h_weight=1.0,\n",
    "    h_min=1.0,\n",
    "    w_survive=0.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-robust_econ\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le 1er épisode\n",
    "    video_length=0,                       # épisode complet\n",
    ")\n",
    "\n",
    "# si tu avais sauvegardé :\n",
    "# model_robust_econ = SAC.load(\"sac_walker2d_robust_econ\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_robust_econ.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2366cc",
   "metadata": {},
   "source": [
    "The groupings:\n",
    "\n",
    "* Speed vs. energy: R1 or R7\n",
    "* Target speed: R2\n",
    "* Posture / avoiding falls: R3 or R6\n",
    "* Smooth actions: R4 and R5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walker2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
