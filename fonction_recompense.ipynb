{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7d006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_FNS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43badf",
   "metadata": {},
   "source": [
    "### Fonctions de récompense pour Walker2d-v5\n",
    "\n",
    "Dans ce projet, nous voulons étudier l'effet de différentes fonctions de récompense pour l'environnement `Walker2d-v5` (Gymnasium / MuJoCo). Dans la version de base, la récompense à chaque pas est la somme de trois termes :\n",
    "\n",
    "- **healthy_reward** : bonus de survie lorsque le robot reste dans une zone considérée comme \"saine\" ;\n",
    "- **forward_reward** : récompense proportionnelle à la vitesse vers l'avant (déplacement en x par unité de temps) ;\n",
    "- **ctrl_cost** : coût quadratique sur les actions (pénalise les torques trop élevés).\n",
    "\n",
    "La récompense totale est donc :\n",
    "\n",
    "> reward = healthy_reward + forward_reward − ctrl_cost\n",
    "\n",
    "et `info` contient les termes individuels sous les clés  \n",
    "`\"reward_forward\"`, `\"reward_ctrl\"`, `\"reward_survive\"`.  \n",
    "(Cf. la documentation officielle de `Walker2d-v5`.)\n",
    "\n",
    "Dans le cadre du *reward shaping*, on modifie cette fonction de récompense pour guider l'apprentissage, par exemple en ajoutant des termes de posture, de vitesse cible ou de lissage des actions. Ce type de modification est étudié de manière théorique dans l'article classique de Ng, Harada et Russell, *\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\"* (ICML 1999), qui montre dans quels cas certaines transformations de la récompense conservent la politique optimale. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "Le code ci-dessous implémente plusieurs variantes de récompense pour `Walker2d-v5` sous forme d'un wrapper Gymnasium, de façon à pouvoir sélectionner facilement une fonction de récompense et entraîner un agent avec celle-ci.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26155d91",
   "metadata": {},
   "source": [
    "### Fonction de récompense 1 : vitesse – énergie – survie\n",
    "\n",
    "Dans l'environnement `Walker2d-v5` (Gymnasium / MuJoCo), la récompense par défaut est décomposée en trois termes :\n",
    "\n",
    "- `reward_forward` : récompense pour la vitesse vers l'avant ;\n",
    "- `reward_ctrl` : coût de contrôle (négatif), proportionnel au carré des actions ;\n",
    "- `reward_survive` : bonus de survie tant que le robot reste dans un état \"sain\".\n",
    "\n",
    "Voir la documentation de `Walker2d-v5` qui précise cette décomposition de la récompense dans le dictionnaire `info`.   \n",
    "\n",
    "Pour notre première fonction de récompense, nous définissons une combinaison linéaire de ces trois termes :\n",
    "\n",
    "$\n",
    "r_t = w_{\\text{forward}} \\cdot \\text{reward\\_forward}\n",
    "    + w_{\\text{ctrl}} \\cdot \\text{reward\\_ctrl}\n",
    "    + w_{\\text{survive}} \\cdot \\text{reward\\_survive}\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $w_{\\text{forward}}$ contrôle l'importance de la vitesse ;\n",
    "- $w_{\\text{ctrl}}$ contrôle l'importance de l'énergie consommée (comme `reward_ctrl` est négatif, un poids positif signifie \"on pénalise l'énergie\") ;\n",
    "- $w_{\\text{survive}}$ contrôle l'importance du bonus de survie.\n",
    "\n",
    "En faisant varier ces poids, on peut étudier le compromis entre **vitesse**, **consommation d'énergie** et **stabilité** (survie) du marcheur, ce qui est particulièrement intéressant dans un contexte de perturbations (bruit d'observation, randomisation des conditions initiales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73dce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def reward_speed_energy(\n",
    "    info: Dict[str, Any],\n",
    "    w_forward: float = 1.0,\n",
    "    w_ctrl: float = 1.0,\n",
    "    w_survive: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fonction de récompense 1 : combinaison vitesse / énergie / survie\n",
    "    pour Walker2d-v5.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    info : dict\n",
    "        Le dictionnaire `info` renvoyé par env.step(action).\n",
    "        On suppose qu'il contient les clés :\n",
    "        - \"reward_forward\"\n",
    "        - \"reward_ctrl\"\n",
    "        - \"reward_survive\"\n",
    "    w_forward : float\n",
    "        Poids de la récompense de vitesse (reward_forward).\n",
    "    w_ctrl : float\n",
    "        Poids du coût de contrôle (reward_ctrl).\n",
    "        Attention : reward_ctrl est déjà négatif dans Walker2d.\n",
    "        Un w_ctrl > 0 correspond donc à une pénalisation de l'énergie.\n",
    "    w_survive : float\n",
    "        Poids du bonus de survie (reward_survive).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    float\n",
    "        La récompense scalaire r_t.\n",
    "    \"\"\"\n",
    "    forward = float(info.get(\"reward_forward\", 0.0))\n",
    "    ctrl = float(info.get(\"reward_ctrl\", 0.0))         # déjà négatif\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    reward = (\n",
    "        w_forward * forward\n",
    "        + w_ctrl * ctrl\n",
    "        + w_survive * survive\n",
    "    )\n",
    "    return reward\n",
    "\n",
    "# obs, base_reward, terminated, truncated, info = env.step(action)\n",
    "# new_reward = reward_speed_energy(\n",
    "#     info,\n",
    "#     w_forward=1.0,\n",
    "#     w_ctrl=1.0,\n",
    "#     w_survive=1.0,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ef535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- 1) Wrapper qui remplace la récompense par reward_speed_energy ---\n",
    "\n",
    "class SpeedEnergyRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, w_forward=1.0, w_ctrl=1.0, w_survive=1.0):\n",
    "        super().__init__(env)\n",
    "        self.w_forward = w_forward\n",
    "        self.w_ctrl = w_ctrl\n",
    "        self.w_survive = w_survive\n",
    "\n",
    "    def step(self, action):\n",
    "        # on appelle l'env \"normal\"\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # on calcule NOTRE récompense à partir de info\n",
    "        new_reward = reward_speed_energy(\n",
    "            info,\n",
    "            w_forward=self.w_forward,\n",
    "            w_ctrl=self.w_ctrl,\n",
    "            w_survive=self.w_survive,\n",
    "        )\n",
    "\n",
    "        # on renvoie obs, new_reward (et pas base_reward)\n",
    "        return obs, new_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --- 2) Création de l'environnement + enregistrement vidéo ---\n",
    "\n",
    "video_folder = \"./videos_speed_energy\"\n",
    "\n",
    "# IMPORTANT : render_mode=\"rgb_array\" pour pouvoir faire une vidéo\n",
    "env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "# on applique notre wrapper de récompense\n",
    "env_wrapped = SpeedEnergyRewardWrapper(\n",
    "    env_base,\n",
    "    w_forward=1.0,\n",
    "    w_ctrl=1.0,\n",
    "    w_survive=1.0,\n",
    ")\n",
    "\n",
    "# on ajoute le wrapper vidéo\n",
    "env = RecordVideo(\n",
    "    env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-speed_energy\",\n",
    "    episode_trigger=lambda ep_id: True,  # filme tous les épisodes\n",
    "    video_length=0,                       # 0 = filme l'épisode complet\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3) Entraînement rapide avec SAC ---\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# nombre de pas tout petit pour tester (à augmenter plus tard)\n",
    "model.learn(total_timesteps=5_000)\n",
    "\n",
    "\n",
    "# --- 4) On filme un épisode avec le modèle entraîné ---\n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "print(f\"Épisode terminé. Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508bb2c",
   "metadata": {},
   "source": [
    "### Fonction de récompense 2 : vitesse cible\n",
    "\n",
    "L'idée de cette récompense est de ne plus dire \"plus vite = mieux\", mais plutôt \"le robot doit marcher à **une vitesse cible** $v^*$\", par exemple $v^* \\in \\{1.0, 2.0\\}$ m/s.\n",
    "\n",
    "On note :\n",
    "\n",
    "- $v_x$ : la vitesse horizontale du torse (dans `Walker2d-v5`, c'est `obs[8]`, la vitesse en x du torse). \n",
    "- $a_t$ : le vecteur d'actions (torques) au temps $t$,\n",
    "- `reward_survive` : le bonus de survie fourni par l'environnement.\n",
    "\n",
    "La récompense est définie par :\n",
    "\n",
    "$\n",
    "r_t = - \\alpha\\, |v_x - v^*|\n",
    "      - \\beta \\, \\|a_t\\|^2\n",
    "      + w_{\\text{survive}} \\cdot \\text{reward\\_survive}\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $\\alpha$ contrôle l'importance d'être proche de la vitesse cible $v^*$,\n",
    "- $\\beta$ contrôle la pénalisation de l'énergie (norme au carré des actions),\n",
    "- $w_{\\text{survive}}$ ajuste l'importance du terme de survie.\n",
    "\n",
    "#### Intérêt sous perturbations\n",
    "\n",
    "Quand on ajoute du bruit (sur les observations ou les conditions initiales), maximiser simplement la vitesse pousse souvent l'agent à **courir de plus en plus vite**, avec des mouvements nerveux et peu stables.\n",
    "\n",
    "Avec cette fonction de récompense à **vitesse cible** :\n",
    "\n",
    "- l'agent est encouragé à maintenir une **vitesse régulière** proche de $v^*$,\n",
    "- on peut mesurer :\n",
    "  - la variance de la vitesse $v_x$ au cours d'un épisode,\n",
    "  - la variance des actions,\n",
    "  - l'énergie consommée ($\\sum_t \\|a_t\\|^2$),\n",
    "\n",
    "ce qui permet de comparer différentes politiques en termes de **stabilité** et de **robustesse** face aux perturbations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cdf28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_target_speed(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    v_target: float = 1.5,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1e-3,\n",
    "    w_survive: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 2 : vitesse cible + coût d'énergie + survie.\n",
    "\n",
    "        r_t = - alpha * |v_x - v_target|\n",
    "              - beta  * ||a_t||^2\n",
    "              + w_survive * reward_survive\n",
    "\n",
    "    - v_x : vitesse en x du torse (obs[8] dans Walker2d-v5)\n",
    "    - a_t : action au temps t\n",
    "    \"\"\"\n",
    "\n",
    "    # Vitesse horizontale du torse.\n",
    "    # D'après la doc Walker2d-v5, obs[8] = vitesse en x du torse.\n",
    "    vx = float(obs[8])\n",
    "\n",
    "    # Norme au carré de l'action (énergie \"dépensée\")\n",
    "    energy = float(np.sum(np.square(action)))\n",
    "\n",
    "    # Terme de survie fourni par l'env (comme pour reward_speed_energy)\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    # Terme de vitesse : on veut que v_x soit proche de v_target\n",
    "    speed_term = -alpha * abs(vx - v_target)\n",
    "\n",
    "    # Terme d'énergie (pénalisation)\n",
    "    energy_term = -beta * energy\n",
    "\n",
    "    # Récompense totale\n",
    "    reward = speed_term + energy_term + w_survive * survive\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd678e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\miniconda3\\envs\\walker2d\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\arthu\\OneDrive\\Documents\\UTC\\Poly mtl\\RL\\Projet\\videos_target_speed folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.2     |\n",
      "|    ep_rew_mean     | -36.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 109      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.27    |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.998    |\n",
      "|    ent_coef_loss   | -0.0208  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8        |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.8     |\n",
      "|    ep_rew_mean     | -37.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 222      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.85    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.964    |\n",
      "|    ent_coef_loss   | -0.365   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 121      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.3     |\n",
      "|    ep_rew_mean     | -34.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 496      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.07    |\n",
      "|    critic_loss     | 0.523    |\n",
      "|    ent_coef        | 0.888    |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 395      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56.2     |\n",
      "|    ep_rew_mean     | -60.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 900      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.3    |\n",
      "|    critic_loss     | 0.63     |\n",
      "|    ent_coef        | 0.788    |\n",
      "|    ent_coef_loss   | -2.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67       |\n",
      "|    ep_rew_mean     | -80.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 1340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.8    |\n",
      "|    critic_loss     | 0.667    |\n",
      "|    ent_coef        | 0.693    |\n",
      "|    ent_coef_loss   | -3.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.5     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 2028     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.6    |\n",
      "|    critic_loss     | 0.762    |\n",
      "|    ent_coef        | 0.569    |\n",
      "|    ent_coef_loss   | -4.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1927     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -97.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 2820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.454    |\n",
      "|    ent_coef_loss   | -6.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 3640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.358    |\n",
      "|    ent_coef_loss   | -8.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 121      |\n",
      "|    ep_rew_mean     | -90.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 4370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.29     |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 132      |\n",
      "|    ep_rew_mean     | -89      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 5285     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5184     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 139      |\n",
      "|    ep_rew_mean     | -82.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 6121     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | -75.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 6903     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.143    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6802     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | -82.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 7855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 4.43     |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7754     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -81.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 8763     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 3.58     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8662     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | -74.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 9396     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.1    |\n",
      "|    critic_loss     | 3.98     |\n",
      "|    ent_coef        | 0.0736   |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9295     |\n",
      "---------------------------------\n",
      "Épisode terminé. Vidéo enregistrée dans : ./videos_target_speed\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- Wrapper pour la récompense \"vitesse cible\" ---\n",
    "\n",
    "class TargetSpeedRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, v_target=1.5, alpha=1.0, beta=1e-3, w_survive=1.0):\n",
    "        super().__init__(env)\n",
    "        self.v_target = v_target\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.w_survive = w_survive\n",
    "\n",
    "    def step(self, action):\n",
    "        # Env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Notre nouvelle récompense\n",
    "        new_reward = reward_target_speed(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            v_target=self.v_target,\n",
    "            alpha=self.alpha,\n",
    "            beta=self.beta,\n",
    "            w_survive=self.w_survive,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --- Création de l'env + enregistrement vidéo ---\n",
    "\n",
    "video_folder = \"./videos_target_speed\"\n",
    "\n",
    "env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "env_wrapped = TargetSpeedRewardWrapper(\n",
    "    env_base,\n",
    "    v_target=1.5,   # vitesse cible en m/s (tu peux tester 1.0, 2.0, etc.)\n",
    "    alpha=1.0,\n",
    "    beta=1e-3,\n",
    "    w_survive=1.0,\n",
    ")\n",
    "\n",
    "env = RecordVideo(\n",
    "    env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-target_speed\",\n",
    "    episode_trigger=lambda ep_id: True,\n",
    "    video_length=0,\n",
    ")\n",
    "\n",
    "# --- Entraînement rapide avec SAC (juste pour tester) ---\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# --- On filme un épisode avec le modèle entraîné ---\n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "print(f\"Épisode terminé. Vidéo enregistrée dans : {video_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f2350",
   "metadata": {},
   "source": [
    "### Fonction de récompense 3 : posture stable\n",
    "\n",
    "Cette récompense vise à encourager non seulement le déplacement vers l'avant, mais aussi une **posture stable** : marcher \"debout et droit\".\n",
    "\n",
    "Dans `Walker2d-v5` :\n",
    "\n",
    "- `info[\"reward_forward\"]` : terme de vitesse vers l'avant,\n",
    "- `info[\"reward_ctrl\"]` : coût de contrôle (négatif, pénalise les grandes actions),\n",
    "- `info[\"reward_survive\"]` : bonus de survie lorsque le robot reste dans une zone \"saine\",\n",
    "- `obs[0]` : hauteur du torse,\n",
    "- `obs[1]` : angle du torse (0 = vertical). \n",
    "La récompense est définie comme :\n",
    "\n",
    "$\n",
    "r_t = w_{\\text{forward}} \\cdot \\text{reward\\_forward}\n",
    "    + w_{\\text{ctrl}} \\cdot \\text{reward\\_ctrl}\n",
    "    + w_{\\text{survive}} \\cdot \\text{reward\\_survive}\n",
    "    - w_h \\, \\max(0, h_{\\text{target}} - h_t)^2\n",
    "    - w_{\\text{angle}} \\, \\theta_t^2\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $h_t$ est la hauteur du torse au temps $t$,\n",
    "- $\\theta_t$ est l'angle du torse (0 = droit),\n",
    "- $h_{\\text{target}}$ est une hauteur cible (par ex. 1.25),\n",
    "- $(w_h$ contrôle l'importance d'être suffisamment haut,\n",
    "- $w_{\\text{angle}}$ contrôle l'importance d'être droit.\n",
    "\n",
    "#### Enjeux et intérêt avec bruit / resets\n",
    "\n",
    "Avec du bruit d'observation et des conditions initiales aléatoires, le walker a tendance à :\n",
    "\n",
    "- se pencher beaucoup vers l'avant ou l'arrière,\n",
    "- s'affaisser (baisser la hauteur du torse) avant de tomber.\n",
    "\n",
    "Cette récompense \"posture stable\" :\n",
    "\n",
    "- pénalise progressivement les **postures dangereuses** (trop penché, trop bas),\n",
    "- fournit un signal de récompense **avant la chute**,\n",
    "- encourage des gaits plus **stables** et donc souvent plus **robustes** aux perturbations.\n",
    "\n",
    "On peut comparer cette récompense à d'autres en mesurant :\n",
    "- la hauteur moyenne du torse,\n",
    "- la variance de l'angle du torse,\n",
    "- le nombre de chutes / terminaisons par épisode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d047b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def reward_posture_stability(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    h_target: float = 1.25,\n",
    "    w_forward: float = 1.0,\n",
    "    w_ctrl: float = 1.0,\n",
    "    w_survive: float = 1.0,\n",
    "    w_h: float = 1.0,\n",
    "    w_angle: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 3 : posture stable\n",
    "\n",
    "    Idée :\n",
    "      - garder les termes classiques (vitesse, coût de contrôle, survie)\n",
    "      - ajouter des termes qui encouragent une posture \"debout et droite\"\n",
    "\n",
    "    r_t = w_forward * reward_forward\n",
    "        + w_ctrl    * reward_ctrl\n",
    "        + w_survive * reward_survive\n",
    "        + height_term\n",
    "        + angle_term\n",
    "\n",
    "    où :\n",
    "      - height_term pénalise si la hauteur du torse est en dessous d'une cible h_target\n",
    "      - angle_term pénalise si l'angle du torse s'éloigne de 0\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Termes classiques de Walker2d ---\n",
    "\n",
    "    forward = float(info.get(\"reward_forward\", 0.0))\n",
    "    ctrl = float(info.get(\"reward_ctrl\", 0.0))         # déjà négatif\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    base_terms = (\n",
    "        w_forward * forward\n",
    "        + w_ctrl * ctrl\n",
    "        + w_survive * survive\n",
    "    )\n",
    "\n",
    "    # --- 2) Termes de posture ---\n",
    "\n",
    "    # D'après la doc Walker2d-v5 :\n",
    "    #   obs[0] = hauteur du torse\n",
    "    #   obs[1] = angle du torse (0 = droit).  :contentReference[oaicite:0]{index=0}\n",
    "    h = float(obs[0])\n",
    "    angle = float(obs[1])\n",
    "\n",
    "    # pénalité si le torse est plus bas que h_target (quadratique)\n",
    "    height_penalty = -w_h * max(0.0, h_target - h) ** 2\n",
    "\n",
    "    # pénalité quadratique sur l'angle (on veut angle ≈ 0)\n",
    "    angle_penalty = -w_angle * angle ** 2\n",
    "\n",
    "    reward = base_terms + height_penalty + angle_penalty\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49b88d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | -5.22    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2286     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 74       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -5.74    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 229      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 157      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.55    |\n",
      "|    critic_loss     | 3.52     |\n",
      "|    ent_coef        | 0.984    |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 56       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -5.27    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 229      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.34    |\n",
      "|    critic_loss     | 3.11     |\n",
      "|    ent_coef        | 0.962    |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 128      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -5.75    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 313      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.25    |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.938    |\n",
      "|    ent_coef_loss   | -0.638   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 212      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | -6.49    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 109      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 446      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.8    |\n",
      "|    critic_loss     | 0.957    |\n",
      "|    ent_coef        | 0.902    |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 345      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | -7.31    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 578      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12      |\n",
      "|    critic_loss     | 0.827    |\n",
      "|    ent_coef        | 0.867    |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 477      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.4     |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 767      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13      |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.82     |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 666      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.2     |\n",
      "|    ep_rew_mean     | -10      |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1061     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.752    |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 960      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43       |\n",
      "|    ep_rew_mean     | -13.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1547     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.655    |\n",
      "|    ent_coef_loss   | -3.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1446     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.6     |\n",
      "|    ep_rew_mean     | -15      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 2183     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.9    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.548    |\n",
      "|    ent_coef_loss   | -5.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2082     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.8     |\n",
      "|    ep_rew_mean     | 4.33     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 2938     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.2    |\n",
      "|    critic_loss     | 5.13     |\n",
      "|    ent_coef        | 0.443    |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2837     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 4241     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.2    |\n",
      "|    critic_loss     | 6.39     |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | -9.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.3     |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 4749     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.5    |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.264    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4648     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 5348     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.8    |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5247     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 29.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 6034     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -40.3    |\n",
      "|    critic_loss     | 5.39     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5933     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 42.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 7081     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.1    |\n",
      "|    critic_loss     | 9.7      |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 119      |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 8082     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.8    |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -9.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7981     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 76       |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 9143     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.5    |\n",
      "|    critic_loss     | 9.24     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -8.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9042     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (pas de vidéo ici) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # PAS de render_mode pour l'entraînement\n",
    "\n",
    "train_env = PostureStabilityRewardWrapper(\n",
    "    train_env_base,\n",
    "    h_target=1.25,\n",
    "    w_forward=1.0,\n",
    "    w_ctrl=1.0,\n",
    "    w_survive=1.0,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "model = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# (optionnel) sauver le modèle\n",
    "model.save(\"sac_walker2d_posture_stable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c713aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\miniconda3\\envs\\walker2d\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\arthu\\OneDrive\\Documents\\UTC\\Poly mtl\\RL\\Projet\\videos_posture_stable folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_posture_stable\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "video_folder = \"./videos_posture_stable\"\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = PostureStabilityRewardWrapper(\n",
    "    eval_env_base,\n",
    "    h_target=1.25,\n",
    "    w_forward=1.0,\n",
    "    w_ctrl=1.0,\n",
    "    w_survive=1.0,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-posture_stable\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le premier épisode\n",
    "    video_length=0,                       # 0 = épisode complet\n",
    ")\n",
    "\n",
    "# si tu as sauvegardé le modèle :\n",
    "# model = SAC.load(\"sac_walker2d_posture_stable\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c000a5b",
   "metadata": {},
   "source": [
    "### Fonction de récompense 4 : actions lisses\n",
    "\n",
    "Cette récompense cherche à éviter les politiques qui réagissent de manière **trop nerveuse** au bruit (observations bruitées, resets aléatoires, etc.).  \n",
    "L'idée est de pénaliser les **changements brusques d'actions** entre deux pas de temps :\n",
    "\n",
    "$\n",
    "r_t = \\text{base\\_reward}_t - \\lambda \\, \\lVert a_t - a_{t-1} \\rVert^2,\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $\\text{base\\_reward}_t$ est la récompense originale de l'environnement `Walker2d-v5` (vitesse + survie − coût de contrôle)   \n",
    "- $a_t$ est l'action au temps t,\n",
    "- $a_{t-1}$ est l'action au temps $t-1$,\n",
    "- $\\lambda$ (noté `lambda_smooth` dans le code) contrôle la force de la pénalisation des changements d'action.\n",
    "\n",
    "#### Enjeux et intérêt avec du bruit\n",
    "\n",
    "Avec du bruit d'observation, les algorithmes RL ont tendance à :\n",
    "\n",
    "- réagir fortement à de petites fluctuations,\n",
    "- produire des actions qui oscillent très vite,\n",
    "- avoir des gaits moins stables et parfois plus coûteux en énergie.\n",
    "\n",
    "La récompense \"actions lisses\" encourage :\n",
    "\n",
    "- des **commandes plus régulières** (moins d'oscillations haute fréquence),\n",
    "- des gaits souvent plus **stables** et plus **robustes** au bruit,\n",
    "- une politique plus \"conservatrice\" dans ses changements d'actions.\n",
    "\n",
    "On peut comparer différentes valeurs de $\\lambda$ (par ex. `1e-3`, `1e-2`, `5e-2`) et mesurer :\n",
    "- la variance des actions,\n",
    "- la consommation d'énergie,\n",
    "- la robustesse aux perturbations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6dcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_smooth_actions(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    base_reward: float,\n",
    "    prev_action,\n",
    "    lambda_smooth: float = 1e-2,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 4 : actions lisses (anti-réaction nerveuse au bruit)\n",
    "\n",
    "    r_t = base_reward_t - lambda_smooth * ||a_t - a_{t-1}||^2\n",
    "\n",
    "    - base_reward_t : récompense originale de l'environnement Walker2d-v5\n",
    "    - a_t           : action actuelle\n",
    "    - a_{t-1}       : action précédente\n",
    "    \"\"\"\n",
    "    # Si on n'a pas encore d'action précédente (premier step), on ne pénalise pas\n",
    "    if prev_action is None:\n",
    "        return float(base_reward)\n",
    "\n",
    "    # Différence entre action actuelle et précédente\n",
    "    delta = action - prev_action\n",
    "\n",
    "    # Norme au carré de la différence (grands changements d'actions = pénalisés)\n",
    "    smooth_penalty = lambda_smooth * float(np.sum(delta ** 2))\n",
    "\n",
    "    new_reward = float(base_reward) - smooth_penalty\n",
    "    return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ef5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class SmoothActionsRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, lambda_smooth: float = 1e-2):\n",
    "        super().__init__(env)\n",
    "        self.lambda_smooth = lambda_smooth\n",
    "        self.prev_action = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Au reset, on n'a pas encore d'action précédente\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Appel de l'env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Calcul de notre récompense \"actions lisses\"\n",
    "        new_reward = reward_smooth_actions(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            base_reward=base_reward,\n",
    "            prev_action=self.prev_action,\n",
    "            lambda_smooth=self.lambda_smooth,\n",
    "        )\n",
    "\n",
    "        # Mise à jour de l'action précédente\n",
    "        self.prev_action = np.array(action, copy=True)\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f13e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -2.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 1648     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 77       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -2.29    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 234      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 153      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.68    |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.985    |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.7     |\n",
      "|    ep_rew_mean     | -1.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 212      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.55    |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.967    |\n",
      "|    ent_coef_loss   | -0.334   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 111      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -0.97    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 108      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 315      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.69    |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.938    |\n",
      "|    ent_coef_loss   | -0.642   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 214      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -0.271   |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 455      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.2    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.899    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 354      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.3     |\n",
      "|    ep_rew_mean     | 0.349    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 583      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.1    |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 482      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.8     |\n",
      "|    ep_rew_mean     | 0.169    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 863      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15      |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.797    |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 762      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.6     |\n",
      "|    ep_rew_mean     | -2.53    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1234     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.3    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.715    |\n",
      "|    ent_coef_loss   | -3.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1133     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45.4     |\n",
      "|    ep_rew_mean     | -4.49    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1634     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.5    |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.637    |\n",
      "|    ent_coef_loss   | -4.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1533     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58       |\n",
      "|    ep_rew_mean     | 3.42     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 2321     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.9    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.526    |\n",
      "|    ent_coef_loss   | -5.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2220     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.5     |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 3410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.3    |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.388    |\n",
      "|    ent_coef_loss   | -7.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.1     |\n",
      "|    ep_rew_mean     | 47.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 4371     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.3    |\n",
      "|    critic_loss     | 3.08     |\n",
      "|    ent_coef        | 0.294    |\n",
      "|    ent_coef_loss   | -9.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4270     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.5     |\n",
      "|    ep_rew_mean     | 67.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 5176     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.1    |\n",
      "|    critic_loss     | 4.72     |\n",
      "|    ent_coef        | 0.234    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5075     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | 82.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 5837     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.4    |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5736     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 94.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 6488     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.8    |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.164    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6387     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 7121     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.6    |\n",
      "|    critic_loss     | 17.2     |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | 115      |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 7706     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.7    |\n",
      "|    critic_loss     | 8.43     |\n",
      "|    ent_coef        | 0.12     |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7605     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | 118      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 8469     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.8    |\n",
      "|    critic_loss     | 8.53     |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -9       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8368     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 9097     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -67.3    |\n",
      "|    critic_loss     | 7.85     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | -9.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8996     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 9917     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.1    |\n",
      "|    critic_loss     | 7.97     |\n",
      "|    ent_coef        | 0.0722   |\n",
      "|    ent_coef_loss   | -5.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9816     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x2963cc2e2f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (sans vidéo) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # pas de render_mode ici\n",
    "train_env = SmoothActionsRewardWrapper(\n",
    "    train_env_base,\n",
    "    lambda_smooth=1e-2,  # tu peux tester différentes valeurs\n",
    ")\n",
    "\n",
    "model_smooth = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_smooth.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# optionnel : sauver le modèle\n",
    "# model_smooth.save(\"sac_walker2d_smooth_actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a09515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_smooth_actions\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_smooth_actions\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "eval_env_wrapped = SmoothActionsRewardWrapper(\n",
    "    eval_env_base,\n",
    "    lambda_smooth=1e-2,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-smooth_actions\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le premier épisode\n",
    "    video_length=0,                       # épisode complet\n",
    ")\n",
    "\n",
    "# Si tu avais sauvegardé : model_smooth = SAC.load(\"sac_walker2d_smooth_actions\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_smooth.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6e4de",
   "metadata": {},
   "source": [
    "### Fonction de récompense 5 : anti-chute progressive\n",
    "\n",
    "L'objectif de cette récompense est de fournir un **signal de danger progressif** quand le walker se rapproche de la chute, au lieu d'avoir seulement une grosse pénalité quand l'épisode se termine.\n",
    "\n",
    "Dans `Walker2d-v5`, l'observation contient notamment :  \n",
    "- `obs[0]` : la hauteur du torse,  \n",
    "- `obs[1]` : l'angle du torse (0 = vertical).  \n",
    "\n",
    "La récompense de base de l'environnement (vitesse vers l'avant, coût de contrôle, survie) est notée $\\text{base\\_reward}_t$.  \n",
    "On définit alors :\n",
    "\n",
    "$\n",
    "r_t = \\text{base\\_reward}_t\n",
    "      - w_h \\, \\max(0, h_{\\text{crit}} - h_t)^2\n",
    "      - w_{\\text{angle}} \\, \\max(0, |\\theta_t| - \\theta_{\\text{crit}})^2\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $h_t$ est la hauteur du torse au temps t,\n",
    "- $\\theta_t$ est l'angle du torse (0 = droit),\n",
    "- $h_{\\text{crit}}$ est une hauteur \"critique\" en dessous de laquelle le robot est proche de se coucher,\n",
    "- $theta_{\\text{crit}}$ est un angle limite au-delà duquel le robot est trop penché,\n",
    "- $w_h$ et $w_{\\text{angle}}$ contrôlent l'importance de ces pénalités.\n",
    "\n",
    "#### Intérêt du shaping \"anti-chute progressive\"\n",
    "\n",
    "Avec cette forme :\n",
    "\n",
    "- les états **dangereux** (trop bas, trop penchés) sont pénalisés **avant** la chute,\n",
    "- l'agent peut apprendre à **se redresser** et **se rattraper** au lieu de simplement tomber,\n",
    "- en présence de bruit (observations bruitées, resets aléatoires), cela aide à apprendre des comportements plus **robustes** :\n",
    "  - moins de chutes,\n",
    "  - plus de temps passé dans des états \"sûrs\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc54d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_anti_fall_progressive(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    base_reward: float,\n",
    "    h_crit: float = 0.9,\n",
    "    angle_crit: float = 0.5,\n",
    "    w_h: float = 5.0,\n",
    "    w_angle: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 5 : anti-chute progressive (shaping autour des états dangereux)\n",
    "\n",
    "    Idée :\n",
    "      - on part de la récompense de base de l'environnement (base_reward)\n",
    "      - on ajoute des pénalités \"douces\" quand le walker se rapproche de la chute :\n",
    "          * torse trop bas  (h < h_crit)\n",
    "          * torse trop penché (|angle| > angle_crit)\n",
    "\n",
    "    Formule :\n",
    "        r_t = base_reward\n",
    "              - w_h     * max(0, h_crit - h_t)^2\n",
    "              - w_angle * max(0, |angle_t| - angle_crit)^2\n",
    "\n",
    "    où :\n",
    "      - h_t      : hauteur du torse au temps t\n",
    "      - angle_t  : angle du torse (0 = vertical)\n",
    "      - h_crit   : hauteur critique en dessous de laquelle on \"approche\" de la chute\n",
    "      - angle_crit : angle limite au-delà duquel le torse est trop penché\n",
    "    \"\"\"\n",
    "\n",
    "    # D'après la doc Walker2d-v5 :\n",
    "    #   obs[0] = hauteur du torse\n",
    "    #   obs[1] = angle du torse (0 = droit).  \n",
    "    h = float(obs[0])\n",
    "    angle = float(obs[1])\n",
    "\n",
    "    # 1) Danger de hauteur : si le torse descend sous h_crit\n",
    "    height_danger = max(0.0, h_crit - h)\n",
    "    height_penalty = w_h * height_danger ** 2\n",
    "\n",
    "    # 2) Danger d'angle : si |angle| dépasse angle_crit\n",
    "    angle_danger = max(0.0, abs(angle) - angle_crit)\n",
    "    angle_penalty = w_angle * angle_danger ** 2\n",
    "\n",
    "    # Récompense finale : base - pénalités\n",
    "    new_reward = float(base_reward) - height_penalty - angle_penalty\n",
    "    return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38a501eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class AntiFallProgressiveRewardWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        h_crit: float = 0.9,\n",
    "        angle_crit: float = 0.5,\n",
    "        w_h: float = 5.0,\n",
    "        w_angle: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.h_crit = h_crit\n",
    "        self.angle_crit = angle_crit\n",
    "        self.w_h = w_h\n",
    "        self.w_angle = w_angle\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Notre récompense \"anti-chute progressive\"\n",
    "        new_reward = reward_anti_fall_progressive(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            base_reward=base_reward,\n",
    "            h_crit=self.h_crit,\n",
    "            angle_crit=self.angle_crit,\n",
    "            w_h=self.w_h,\n",
    "            w_angle=self.w_angle,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f62750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16       |\n",
      "|    ep_rew_mean     | -3.54    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2133     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 64       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16       |\n",
      "|    ep_rew_mean     | -4.66    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 364      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.49    |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.992    |\n",
      "|    ent_coef_loss   | -0.0795  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.4     |\n",
      "|    ep_rew_mean     | -2.95    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 209      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.93    |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.968    |\n",
      "|    ent_coef_loss   | -0.327   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 108      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -2.37    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 281      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.71    |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.947    |\n",
      "|    ent_coef_loss   | -0.543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -2.76    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 381      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.95    |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.919    |\n",
      "|    ent_coef_loss   | -0.849   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 280      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -2.96    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 109      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 515      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.7    |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.883    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 414      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.8     |\n",
      "|    ep_rew_mean     | -3.37    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 105      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 611      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.5    |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.858    |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 510      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.2     |\n",
      "|    ep_rew_mean     | -3.77    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 807      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.4    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.81     |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 706      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.7     |\n",
      "|    ep_rew_mean     | -3.51    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1034     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.9    |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.758    |\n",
      "|    ent_coef_loss   | -2.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 933      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.8     |\n",
      "|    ep_rew_mean     | -5.96    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 1431     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.676    |\n",
      "|    ent_coef_loss   | -3.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1330     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42       |\n",
      "|    ep_rew_mean     | -7.52    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 1850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.9    |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.6      |\n",
      "|    ent_coef_loss   | -4.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 52.8     |\n",
      "|    ep_rew_mean     | -4.01    |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 2534     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.1    |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.495    |\n",
      "|    ent_coef_loss   | -5.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2433     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.2     |\n",
      "|    ep_rew_mean     | 11.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 3393     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -31.7    |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.388    |\n",
      "|    ent_coef_loss   | -7.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3292     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.9     |\n",
      "|    ep_rew_mean     | 33.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 4251     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.8    |\n",
      "|    critic_loss     | 4.4      |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | -9.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.7     |\n",
      "|    ep_rew_mean     | 52       |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 5379     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.4    |\n",
      "|    critic_loss     | 7.07     |\n",
      "|    ent_coef        | 0.221    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5278     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | 67       |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 6067     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -49.4    |\n",
      "|    critic_loss     | 5.64     |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5966     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 84.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 6925     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.6    |\n",
      "|    critic_loss     | 5.59     |\n",
      "|    ent_coef        | 0.146    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6824     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 94.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 7637     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56.4    |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | -9.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7536     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 8280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.5    |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.105    |\n",
      "|    ent_coef_loss   | -9.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8179     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | 114      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 9106     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.4    |\n",
      "|    critic_loss     | 15.4     |\n",
      "|    ent_coef        | 0.0876   |\n",
      "|    ent_coef_loss   | -7.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9005     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 119      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 9968     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.4    |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0727   |\n",
      "|    ent_coef_loss   | -6.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9867     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x2964d521ed0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (sans vidéo) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # pas de render_mode ici\n",
    "\n",
    "train_env = AntiFallProgressiveRewardWrapper(\n",
    "    train_env_base,\n",
    "    h_crit=0.9,\n",
    "    angle_crit=0.5,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "model_anti_fall = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_anti_fall.learn(total_timesteps=10_000)   # à augmenter plus tard\n",
    "\n",
    "# optionnel : sauvegarder\n",
    "# model_anti_fall.save(\"sac_walker2d_anti_fall_progressive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06641d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_anti_fall_progressive\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_anti_fall_progressive\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = AntiFallProgressiveRewardWrapper(\n",
    "    eval_env_base,\n",
    "    h_crit=0.9,\n",
    "    angle_crit=0.5,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-anti_fall\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le 1er épisode\n",
    "    video_length=0,                       # épisode complet\n",
    ")\n",
    "\n",
    "# si tu as sauvegardé :\n",
    "# model_anti_fall = SAC.load(\"sac_walker2d_anti_fall_progressive\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_anti_fall.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c0f49",
   "metadata": {},
   "source": [
    "### Fonction de récompense 6 : anti-chute progressive (shaping autour des états dangereux)\n",
    "\n",
    "Cette récompense a pour objectif d'aider le walker à **éviter la chute** en pénalisant progressivement les états \"dangereux\", au lieu d'avoir uniquement une grosse punition quand l'épisode se termine.\n",
    "\n",
    "Dans `Walker2d-v5`, on utilise notamment :\n",
    "\n",
    "- `base_reward` : la récompense de base de l'environnement (vitesse vers l'avant, survie, coût de contrôle)  \n",
    "- `obs[0]` : la hauteur du torse  \n",
    "- `obs[1]` : l'angle du torse (0 = torse vertical)\n",
    "\n",
    "On introduit deux seuils :\n",
    "\n",
    "- $h_{\\text{crit}}$ : hauteur critique en dessous de laquelle le robot est considéré comme trop bas (proche de se coucher)  \n",
    "- $\\theta_{\\text{crit}}$ : angle critique au-delà duquel le torse est trop penché\n",
    "\n",
    "La récompense est définie comme :\n",
    "\n",
    "$\n",
    "r_t = \\text{base\\_reward}_t\n",
    "      - w_h \\, \\max(0, h_{\\text{crit}} - h_t)^2\n",
    "      - w_{\\text{angle}} \\, \\max(0, |\\theta_t| - \\theta_{\\text{crit}})^2\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $h_t$ est la hauteur du torse au temps t,\n",
    "- $\\theta_t$ est l'angle du torse,\n",
    "- $w_h$ contrôle l'importance de la pénalisation de la hauteur dangereuse,\n",
    "- $w_{\\text{angle}}$ contrôle l'importance de la pénalisation de l'angle dangereux.\n",
    "\n",
    "#### Intérêt de ce shaping\n",
    "\n",
    "- Les états **proches de la chute** (trop bas, trop penchés) sont pénalisés avant la fin de l’épisode.  \n",
    "- L’agent reçoit donc un **signal de danger progressif** qui lui permet d’apprendre à :\n",
    "  - se redresser,\n",
    "  - éviter de s’affaisser,\n",
    "  - corriger sa posture au lieu de simplement tomber.\n",
    "\n",
    "Dans un contexte de **bruit** (observations bruitées, resets aléatoires), cette récompense permet de tester si les algorithmes RL apprennent des comportements plus **robustes**, en réduisant :\n",
    "\n",
    "- le nombre de chutes par épisode,\n",
    "- le temps passé dans des postures dangereuses (hauteur < $h_{\\text{crit}}$, angle > $\\theta_{\\text{crit}}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25238e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_anti_fall_progressive(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    base_reward: float,\n",
    "    h_crit: float = 0.9,\n",
    "    angle_crit: float = 0.5,\n",
    "    w_h: float = 5.0,\n",
    "    w_angle: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 6 : anti-chute progressive (shaping autour des états dangereux)\n",
    "\n",
    "    Idée :\n",
    "      - on part de la récompense de base de l'environnement (base_reward)\n",
    "      - on enlève un bonus quand le walker se rapproche de la chute :\n",
    "          * torse trop bas  (h < h_crit)\n",
    "          * torse trop penché (|angle| > angle_crit)\n",
    "\n",
    "    r_t = base_reward\n",
    "          - w_h     * max(0, h_crit - h_t)^2\n",
    "          - w_angle * max(0, |angle_t| - angle_crit)^2\n",
    "    \"\"\"\n",
    "\n",
    "    # D'après Walker2d-v5 :\n",
    "    #   obs[0] = hauteur du torse\n",
    "    #   obs[1] = angle du torse (0 = vertical)\n",
    "    h = float(obs[0])\n",
    "    angle = float(obs[1])\n",
    "\n",
    "    # Danger de hauteur : si le torse descend sous h_crit\n",
    "    height_danger = max(0.0, h_crit - h)\n",
    "    height_penalty = w_h * height_danger**2\n",
    "\n",
    "    # Danger d'angle : si |angle| dépasse angle_crit\n",
    "    angle_danger = max(0.0, abs(angle) - angle_crit)\n",
    "    angle_penalty = w_angle * angle_danger**2\n",
    "\n",
    "    new_reward = float(base_reward) - height_penalty - angle_penalty\n",
    "    return new_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf3ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class AntiFallProgressiveRewardWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        h_crit: float = 0.9,\n",
    "        angle_crit: float = 0.5,\n",
    "        w_h: float = 5.0,\n",
    "        w_angle: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.h_crit = h_crit\n",
    "        self.angle_crit = angle_crit\n",
    "        self.w_h = w_h\n",
    "        self.w_angle = w_angle\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        # step de l'env original\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # récompense 6 : anti-chute progressive\n",
    "        new_reward = reward_anti_fall_progressive(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            base_reward=base_reward,\n",
    "            h_crit=self.h_crit,\n",
    "            angle_crit=self.angle_crit,\n",
    "            w_h=self.w_h,\n",
    "            w_angle=self.w_angle,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aafcdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.2     |\n",
      "|    ep_rew_mean     | 0.436    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 1964     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 65       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.5     |\n",
      "|    ep_rew_mean     | 1.11     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 269      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 132      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.24    |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.991    |\n",
      "|    ent_coef_loss   | -0.0907  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.4     |\n",
      "|    ep_rew_mean     | 0.182    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 197      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.34    |\n",
      "|    critic_loss     | 3.01     |\n",
      "|    ent_coef        | 0.972    |\n",
      "|    ent_coef_loss   | -0.291   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 96       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | 4        |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 392      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.6    |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.916    |\n",
      "|    ent_coef_loss   | -0.869   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 291      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.7     |\n",
      "|    ep_rew_mean     | 4.91     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 634      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.7    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.853    |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 533      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.6     |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 80       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 927      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 0.978    |\n",
      "|    ent_coef        | 0.782    |\n",
      "|    ent_coef_loss   | -2.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 826      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.3     |\n",
      "|    ep_rew_mean     | 0.0992   |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1408     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.681    |\n",
      "|    ent_coef_loss   | -3.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1307     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2362     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.6    |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.519    |\n",
      "|    ent_coef_loss   | -5.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2261     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.5     |\n",
      "|    ep_rew_mean     | 54.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 3079     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.3    |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.424    |\n",
      "|    ent_coef_loss   | -6.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2978     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 84.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 4315     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.2    |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.3      |\n",
      "|    ent_coef_loss   | -9.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4214     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 116      |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 5969     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.4    |\n",
      "|    critic_loss     | 4.04     |\n",
      "|    ent_coef        | 0.189    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5868     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | 119      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 6856     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.3    |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6755     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 147      |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 7628     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.6    |\n",
      "|    critic_loss     | 7.74     |\n",
      "|    ent_coef        | 0.121    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7527     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 8451     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.4    |\n",
      "|    critic_loss     | 8.09     |\n",
      "|    ent_coef        | 0.0982   |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8350     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.6    |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0743   |\n",
      "|    ent_coef_loss   | -7.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1f5e77c3a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (pas de vidéo ici) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")  # pas de render_mode\n",
    "\n",
    "train_env = AntiFallProgressiveRewardWrapper(\n",
    "    train_env_base,\n",
    "    h_crit=0.9,\n",
    "    angle_crit=0.5,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "model_anti_fall = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_anti_fall.learn(total_timesteps=10_000)  # à augmenter plus tard\n",
    "\n",
    "# optionnel :\n",
    "# model_anti_fall.save(\"sac_walker2d_anti_fall_progressive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235ff863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\miniconda3\\envs\\walker2d\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\arthu\\OneDrive\\Documents\\UTC\\Poly mtl\\RL\\Projet\\videos_anti_fall_progressive folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_anti_fall_progressive\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_anti_fall_progressive\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = AntiFallProgressiveRewardWrapper(\n",
    "    eval_env_base,\n",
    "    h_crit=0.9,\n",
    "    angle_crit=0.5,\n",
    "    w_h=5.0,\n",
    "    w_angle=1.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-anti_fall\",\n",
    "    episode_trigger=lambda ep_id: True,\n",
    "    video_length=0,  # épisode complet\n",
    ")\n",
    "\n",
    "# si tu avais sauvegardé :\n",
    "# model_anti_fall = SAC.load(\"sac_walker2d_anti_fall_progressive\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_anti_fall.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf83f60",
   "metadata": {},
   "source": [
    "### Fonction de récompense 7 : marche robuste & économe (multi-objectif simple)\n",
    "\n",
    "Cette récompense cherche à combiner explicitement trois objectifs :\n",
    "\n",
    "1. **Aller vers l'avant** (vitesse),\n",
    "2. **Consommer peu d'énergie** (actions modérées),\n",
    "3. **Rester suffisamment haut** (posture globale stable).\n",
    "\n",
    "Dans `Walker2d-v5` :\n",
    "\n",
    "- `obs[8]` correspond à la vitesse en x du torse,\n",
    "- `obs[0]` correspond à la hauteur du torse,\n",
    "- l'énergie consommée peut être approximée par $\\|a_t\\|^2$ (norme au carré de l'action).\n",
    "\n",
    "On définit :\n",
    "\n",
    "$\n",
    "r_t = w_v \\, v_x\n",
    "      - w_E \\, \\|a_t\\|^2\n",
    "      + w_h \\, \\max(0, h_t - h_{\\min})\n",
    "      + w_{\\text{survive}} \\cdot \\text{reward\\_survive}\n",
    "$\n",
    "\n",
    "où :\n",
    "\n",
    "- $v_x$ est la vitesse horizontale du torse,\n",
    "- $\\|a_t\\|^2$ mesure l'énergie instantanée des actions,\n",
    "- $h_t$ est la hauteur du torse,\n",
    "- $h_{\\min}$ est une hauteur minimale souhaitée (par ex. 1.0),\n",
    "- $w_v$ (`v_weight`) règle l’importance de la vitesse,\n",
    "- $w_E$ (`energy_weight`) règle l’importance de l’économie d’énergie,\n",
    "- $w_h$ (`h_weight`) règle l’importance de rester haut,\n",
    "- $w_{\\text{survive}}$ (`w_survive`) permet éventuellement de garder un terme de survie.\n",
    "\n",
    "#### Intérêt pour la robustesse\n",
    "\n",
    "Cette récompense permet d'étudier un compromis clair entre :\n",
    "\n",
    "- **performance** (distance parcourue, vitesse moyenne),\n",
    "- **coût énergétique** (somme des \\(\\|a_t\\|^2\\)),\n",
    "- **stabilité globale** (hauteur moyenne du torse).\n",
    "\n",
    "Sous perturbations (bruit d'observation, randomisation des resets), on peut comparer différentes politiques en observant :\n",
    "\n",
    "- la distance parcourue,\n",
    "- l'énergie totale consommée,\n",
    "- la hauteur moyenne et le nombre de chutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091fee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "def reward_robust_econ(\n",
    "    obs,\n",
    "    action,\n",
    "    info: Dict[str, Any],\n",
    "    v_weight: float = 1.0,\n",
    "    energy_weight: float = 1e-3,\n",
    "    h_weight: float = 1.0,\n",
    "    h_min: float = 1.0,\n",
    "    w_survive: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Récompense 7 : marche robuste & économe (multi-objectif simple)\n",
    "\n",
    "    Objectif : combiner trois critères :\n",
    "      - vitesse vers l'avant (vx)\n",
    "      - énergie consommée (||a_t||^2)\n",
    "      - hauteur du torse (h)\n",
    "\n",
    "    r_t = v_weight     * v_x\n",
    "        - energy_weight * ||a_t||^2\n",
    "        + h_weight     * max(0, h_t - h_min)\n",
    "        + w_survive    * reward_survive\n",
    "\n",
    "    où :\n",
    "      - v_x     : vitesse en x du torse (obs[8] dans Walker2d-v5)\n",
    "      - a_t     : action au temps t\n",
    "      - h_t     : hauteur du torse (obs[0])\n",
    "      - h_min   : hauteur minimale désirée\n",
    "    \"\"\"\n",
    "\n",
    "    # Vitesse horizontale (gait plus rapide)\n",
    "    vx = float(obs[8])          # velocity of x-coordinate of torso\n",
    "\n",
    "    # Énergie des actions\n",
    "    energy = float(np.sum(np.square(action)))\n",
    "\n",
    "    # Hauteur du torse\n",
    "    h = float(obs[0])\n",
    "\n",
    "    # Terme de survie de l'env (optionnel)\n",
    "    survive = float(info.get(\"reward_survive\", 0.0))\n",
    "\n",
    "    # Terme de hauteur : on récompense si h > h_min\n",
    "    height_term = h_weight * max(0.0, h - h_min)\n",
    "\n",
    "    reward = (\n",
    "        v_weight * vx\n",
    "        - energy_weight * energy\n",
    "        + height_term\n",
    "        + w_survive * survive\n",
    "    )\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ddb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class RobustEconRewardWrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        v_weight: float = 1.0,\n",
    "        energy_weight: float = 1e-3,\n",
    "        h_weight: float = 1.0,\n",
    "        h_min: float = 1.0,\n",
    "        w_survive: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.v_weight = v_weight\n",
    "        self.energy_weight = energy_weight\n",
    "        self.h_weight = h_weight\n",
    "        self.h_min = h_min\n",
    "        self.w_survive = w_survive\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        # step de l'env de base\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # récompense 7 : marche robuste & économe\n",
    "        new_reward = reward_robust_econ(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            info=info,\n",
    "            v_weight=self.v_weight,\n",
    "            energy_weight=self.energy_weight,\n",
    "            h_weight=self.h_weight,\n",
    "            h_min=self.h_min,\n",
    "            w_survive=self.w_survive,\n",
    "        )\n",
    "\n",
    "        return obs, new_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e9c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -17      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2079     |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 79       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -14.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 220      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 157      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.11    |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.983    |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 56       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -15.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 285      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.36    |\n",
      "|    critic_loss     | 0.975    |\n",
      "|    ent_coef        | 0.946    |\n",
      "|    ent_coef_loss   | -0.557   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 184      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.8     |\n",
      "|    ep_rew_mean     | -18.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 460      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.87    |\n",
      "|    critic_loss     | 0.789    |\n",
      "|    ent_coef        | 0.898    |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.5     |\n",
      "|    ep_rew_mean     | -26.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 770      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.1    |\n",
      "|    critic_loss     | 0.68     |\n",
      "|    ent_coef        | 0.819    |\n",
      "|    ent_coef_loss   | -1.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.8     |\n",
      "|    ep_rew_mean     | -38.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.5    |\n",
      "|    critic_loss     | 0.499    |\n",
      "|    ent_coef        | 0.729    |\n",
      "|    ent_coef_loss   | -2.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.1     |\n",
      "|    ep_rew_mean     | -47.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 1824     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.604    |\n",
      "|    ent_coef_loss   | -4.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1723     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.8     |\n",
      "|    ep_rew_mean     | -40.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2456     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.8    |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.505    |\n",
      "|    ent_coef_loss   | -5.96    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2355     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.5     |\n",
      "|    ep_rew_mean     | -19.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 3475     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.9    |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.377    |\n",
      "|    ent_coef_loss   | -7.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3374     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | -2.05    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 4326     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.4    |\n",
      "|    critic_loss     | 2.93     |\n",
      "|    ent_coef        | 0.296    |\n",
      "|    ent_coef_loss   | -9.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4225     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | 13.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 85       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 5350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.4    |\n",
      "|    critic_loss     | 8.35     |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5249     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 6222     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -42.1    |\n",
      "|    critic_loss     | 4.44     |\n",
      "|    ent_coef        | 0.175    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6121     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 138      |\n",
      "|    ep_rew_mean     | 38.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -44.6    |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 141      |\n",
      "|    ep_rew_mean     | 47.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 7904     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.8    |\n",
      "|    critic_loss     | 5.82     |\n",
      "|    ent_coef        | 0.113    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7803     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 54       |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 8624     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.7    |\n",
      "|    critic_loss     | 6.69     |\n",
      "|    ent_coef        | 0.094    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8523     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 148      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 9478     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.3    |\n",
      "|    critic_loss     | 6.49     |\n",
      "|    ent_coef        | 0.076    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9377     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1f593389660>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "# --- ENV D'ENTRAÎNEMENT (pas de vidéo ici) ---\n",
    "\n",
    "train_env_base = gym.make(\"Walker2d-v5\")\n",
    "\n",
    "train_env = RobustEconRewardWrapper(\n",
    "    train_env_base,\n",
    "    v_weight=1.0,\n",
    "    energy_weight=1e-3,\n",
    "    h_weight=1.0,\n",
    "    h_min=1.0,\n",
    "    w_survive=0.0,  # tu peux tester 0.0 ou 1.0\n",
    ")\n",
    "\n",
    "model_robust_econ = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "model_robust_econ.learn(total_timesteps=10_000)  # à augmenter plus tard\n",
    "\n",
    "# optionnel :\n",
    "# model_robust_econ.save(\"sac_walker2d_robust_econ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3647bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vidéo enregistrée dans : ./videos_robust_econ\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_folder = \"./videos_robust_econ\"\n",
    "\n",
    "# --- ENV D'ÉVALUATION AVEC VIDÉO ---\n",
    "\n",
    "eval_env_base = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "eval_env_wrapped = RobustEconRewardWrapper(\n",
    "    eval_env_base,\n",
    "    v_weight=1.0,\n",
    "    energy_weight=1e-3,\n",
    "    h_weight=1.0,\n",
    "    h_min=1.0,\n",
    "    w_survive=0.0,\n",
    ")\n",
    "\n",
    "eval_env = RecordVideo(\n",
    "    eval_env_wrapped,\n",
    "    video_folder=video_folder,\n",
    "    name_prefix=\"walker2d-robust_econ\",\n",
    "    episode_trigger=lambda ep_id: True,  # on filme le 1er épisode\n",
    "    video_length=0,                       # épisode complet\n",
    ")\n",
    "\n",
    "# si tu avais sauvegardé :\n",
    "# model_robust_econ = SAC.load(\"sac_walker2d_robust_econ\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model_robust_econ.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "eval_env.close()\n",
    "print(f\"Vidéo enregistrée dans : {video_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2366cc",
   "metadata": {},
   "source": [
    "les gregroupement : \n",
    "\n",
    "- Vitesse vs énergie : R1 ou R7\n",
    "- Vitesse cible : R2\n",
    "- Posture / éviter de tomber : R3 ou R6\n",
    "- Actions lisses : R4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walker2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
